[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Homework #2 - Scrapy/index.html",
    "href": "posts/Homework #2 - Scrapy/index.html",
    "title": "Homework 2 - Scrapy",
    "section": "",
    "text": "Introduction\nIn this blog post, I will be giving a tutorial on how to write a spider in scrapy to scrape information from a movie database. More specifically, I will be demonstrating using Christopher Nolan’s “The Dark Knight”, one of my favorite movies. Using the data that I scrape, I will create a chart to suggest movies based on the actors in “The Dark Knight”.\n\n\nWriting the movie database parse() function\nFirst we write the initial parse() function, shown below:\n\ndef parse(self, response):\n        cast_crew_url = f\"https://www.themoviedb.org/movie/155-the-dark-knight/cast\"\n        yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\nThis method works by saving the url of the full cast and crew page of the movie website. I use scrapy’s built in functions to go into this website and call a second parse function that will save all of the nanmes of those that acted in the movie. The implementation of the second parse function is below.\n\n\nparse_full_credits function implementation\n\ndef parse_full_credits(self, response):\n        for cast_member in response.css(\"ol.people.credits li\"):\n            actor_url = cast_member.css('a::attr(href)').get()\n\n            if actor_url:\n                yield scrapy.Request(url=response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFirstly, I iterated through all of the actors by using the css identifier of only actors by looking at the html code in the movie cast website. While iterating, I saved each actor’s unique page url in a variable. Using that url variable, I used Scrapy’s request function to go into the actor’s page.\nIn this page, I called a third parse function.\n\n\nparse_actor_page function implementation\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('div.title h2.title a::text').get()\n\n        acting_roles = response.css('div.credits_list h3.zero:contains(\"Acting\")')\n        \n        if acting_roles:\n            for acting_role in acting_roles.xpath('./following-sibling::table[@class=\"card credits\"]//table[@class=\"credit_group\"]'):\n                movie_name = acting_role.css('td.role a.tooltip bdi::text').get()\n\n                if movie_name:\n                    yield {\n                        'actor': actor_name,\n                        'movie_or_TV_name': movie_name,\n                    }\n\nI implemented this by first getting the actor’s name by using the css identifier from the website.\nNext, I sorted through only this actor’s acting roles by using the css identifier for acting roles, which was “Zero”. I iterated through all of these acting roles, and got each movie name, and yielded them into a dictionary of the actor’s name and the movies they’ve acted in."
  },
  {
    "objectID": "posts/Palmer Penguins/index.html",
    "href": "posts/Palmer Penguins/index.html",
    "title": "Palmer Penguins Tutorial",
    "section": "",
    "text": "Blog Post URL https://andrewshan214.github.io/PIC16B/posts/Palmer%20Penguins/\nIn this blog post, I will be giving a short tutorial on how to make a simple data visualization of the “Palmer Penguins” data set. More specifically, I will be showing a simple and informative visualization as a scatter plot to visualize the relationship between penguin culmen length and culmen depth.\nFirst, we load the necessary packages and data, as shown below.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = \"/Users/andrewhan/PIC16B/posts/Palmer Penguins/palmer_penguins.csv\"\npenguins = pd.read_csv(data)\n\nNext, we will filter our data set to drop any entries with NaN values for their culmen length and/or depth.\nUsing that filtered data, we will group the data based on those categories. Then we will plot the data using the matplotlib package. Using key words, we can choose the type of chart we want, as well as the colors. In this case, I went with a scatter plot with blue dots.\nAfter adding the titles and labels, we can display our data visualization!\n\npenguins_filtered = penguins.dropna(subset = ['Culmen Length (mm)', 'Culmen Depth (mm)'])\n\nplt.figure(figsize=(10, 6))\nplt.scatter(penguins['Culmen Length (mm)'], penguins['Culmen Depth (mm)'], c='blue', alpha=0.7)\nplt.title('Scatter Plot of Culmen Length vs Culmen Depth')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, there is scatter plot with all of our data points (penguins) plotted with their respective culmen length and depth. It seems that there is no identifiable trend between their length and depth measurements as there are many penguins with both proportionally larger culmen lengths and culmen depths."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello classmates! I’m a third year Mathematics/Economics Major, with a specialization in Computing.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC16B",
    "section": "",
    "text": "Homework 2 - Scrapy\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Tutorial\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nAndrew Han\n\n\n\n\n\n\nNo matching items"
  }
]