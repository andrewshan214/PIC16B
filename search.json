[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index 2.html",
    "href": "index 2.html",
    "title": "Homework 2 - Scrapy",
    "section": "",
    "text": "Introduction\nhttps://andrewshan214.github.io/PIC16B/posts/Homework%20#2%20-%20Scrapy/index.ipynb\nIn this blog post, I will be giving a tutorial on how to write a spider in scrapy to scrape information from a movie database. More specifically, I will be demonstrating using Christopher Nolan’s “The Dark Knight”, one of my favorite movies. Using the data that I scrape, I will create a chart to suggest movies based on the actors in “The Dark Knight”.\n\n\nWriting the movie database parse() function\nFirst we write the initial parse() function, shown below:\n\ndef parse(self, response):\n        cast_crew_url = f\"https://www.themoviedb.org/movie/155-the-dark-knight/cast\"\n        yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\nThis method works by saving the url of the full cast and crew page of the movie website. I use scrapy’s built in functions to go into this website and call a second parse function that will save all of the nanmes of those that acted in the movie. The implementation of the second parse function is below.\n\n\nparse_full_credits function implementation\n\ndef parse_full_credits(self, response):\n        for cast_member in response.css(\"ol.people.credits li\"):\n            actor_url = cast_member.css('a::attr(href)').get()\n\n            if actor_url:\n                yield scrapy.Request(url=response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFirstly, I iterated through all of the actors by using the css identifier of only actors by looking at the html code in the movie cast website. While iterating, I saved each actor’s unique page url in a variable. Using that url variable, I used Scrapy’s request function to go into the actor’s page.\nIn this page, I called a third parse function.\n\n\nparse_actor_page function implementation\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('div.title h2.title a::text').get()\n\n        acting_roles = response.css('div.credits_list h3.zero:contains(\"Acting\")')\n        \n        if acting_roles:\n            for acting_role in acting_roles.xpath('./following-sibling::table[@class=\"card credits\"]//table[@class=\"credit_group\"]'):\n                movie_name = acting_role.css('td.role a.tooltip bdi::text').get()\n\n                if movie_name:\n                    yield {\n                        'actor': actor_name,\n                        'movie_or_TV_name': movie_name,\n                    }\n\nI implemented this by first getting the actor’s name by using the css identifier from the website.\nNext, I sorted through only this actor’s acting roles by using the css identifier for acting roles, which was “Zero”. I iterated through all of these acting roles, and got each movie name, and yielded them into a dictionary of the actor’s name and the movies they’ve acted in."
  },
  {
    "objectID": "posts/Homework #2 - Scrapy/index.html",
    "href": "posts/Homework #2 - Scrapy/index.html",
    "title": "Homework 2 - Scrapy",
    "section": "",
    "text": "Introduction\nhttps://andrewshan214.github.io/PIC16B/posts/Homework%20#2%20-%20Scrapy/index.ipynb\nIn this blog post, I will be giving a tutorial on how to write a spider in scrapy to scrape information from a movie database. More specifically, I will be demonstrating using Christopher Nolan’s “The Dark Knight”, one of my favorite movies. Using the data that I scrape, I will create a chart to suggest movies based on the actors in “The Dark Knight”.\n\n\nWriting the movie database parse() function\nFirst we write the initial parse() function, shown below:\n\ndef parse(self, response):\n        cast_crew_url = f\"https://www.themoviedb.org/movie/155-the-dark-knight/cast\"\n        yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\nThis method works by saving the url of the full cast and crew page of the movie website. I use scrapy’s built in functions to go into this website and call a second parse function that will save all of the nanmes of those that acted in the movie. The implementation of the second parse function is below.\n\n\nparse_full_credits function implementation\n\ndef parse_full_credits(self, response):\n        for cast_member in response.css(\"ol.people.credits li\"):\n            actor_url = cast_member.css('a::attr(href)').get()\n\n            if actor_url:\n                yield scrapy.Request(url=response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFirstly, I iterated through all of the actors by using the css identifier of only actors by looking at the html code in the movie cast website. While iterating, I saved each actor’s unique page url in a variable. Using that url variable, I used Scrapy’s request function to go into the actor’s page.\nIn this page, I called a third parse function.\n\n\nparse_actor_page function implementation\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('div.title h2.title a::text').get()\n\n        acting_roles = response.css('div.credits_list h3.zero:contains(\"Acting\")')\n        \n        if acting_roles:\n            for acting_role in acting_roles.xpath('./following-sibling::table[@class=\"card credits\"]//table[@class=\"credit_group\"]'):\n                movie_name = acting_role.css('td.role a.tooltip bdi::text').get()\n\n                if movie_name:\n                    yield {\n                        'actor': actor_name,\n                        'movie_or_TV_name': movie_name,\n                    }\n\nI implemented this by first getting the actor’s name by using the css identifier from the website.\nNext, I sorted through only this actor’s acting roles by using the css identifier for acting roles, which was “Zero”. I iterated through all of these acting roles, and got each movie name, and yielded them into a dictionary of the actor’s name and the movies they’ve acted in."
  },
  {
    "objectID": "posts/Homework3-Message_Bank/index.html",
    "href": "posts/Homework3-Message_Bank/index.html",
    "title": "Message bank Tutorial",
    "section": "",
    "text": "Introduction\nThe url for this blog is: https://andrewshan214.github.io/PIC16B/\nThe url for the github repo is: https://github.com/andrewshan214/PIC16B\nIn this blog post, I will be giving a short tutorial on how to make a simple message bank using Python’s Flask library, as well as incorporating HTML and CSS files to develop a webpage\nFirst, we create the necessary files. To start we will create our app.py file, which will be a Python script file that will be running the Flask library tasks. Please see below.\n\nfrom flask import Flask, render_template, request\nfrom flask import redirect, url_for, abort\nfrom flask import g\nimport sqlite3\n\napp = Flask(__name__)\n\n\n\nGoing through app.py file\nAlong with importing the necessary libraries, we must define a few functions and databases for our message bank. We create a base page by simply rendering the accompanying base.html file.\nPlease see the comments above each function to understand what each function is doing in the context of the webpage.\n\n# Create a sqlite3 database to handle all of the submitted messages\nDATABASE = 'messages_db.sqlite'\n\n# This function checks for the existence of a message_db in the global 'g' object. It then closes the database connection if it sees its existence.\n@app.teardown_appcontext\ndef close_db(error):\n    if hasattr(g, 'message_db'):\n        g.message_db.close()\n\n@app.route('/') # @app.route essentially tells the code what the end of the url of this particular page would be. \n# Because this is the base 'Home' page, we use a simple '/' to conclude the url.\ndef base():\n    #Uses Flask's render_template function to render the base.html file\n    return render_template('base.html')\n\n\n\nConnection between app.py and each page’s html file.\nAfter defining a base() function, we must accompany it with an html file to tell the website what to display on this page. See below for the code for base.html.\n\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} - PIC16B Website&lt;/title&gt;\n&lt;nav&gt;\n    &lt;h1&gt;PIC16B Message Bank!&lt;/h1&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('base') }}\"&gt;Main page&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('submit_message')}}\"&gt;Submit Message&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('render_view_template') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;nav&gt;\n&lt;section class=\"content\"&gt;\n    &lt;header&gt;\n    {% block header %}{% endblock %}\n    &lt;/header&gt;\n    {% block content %}{% endblock %}\n&lt;/section&gt;\n\n\n\nHTML explanation\nHTML lines begin with ‘&lt; keyword&gt;’ to indicate the purpose of each line. The line &lt; link rel=“stylesheet” href=“{{ url_for(‘static’, filename=‘style.css’) }}”&gt; indicates to the program to link the entire page to the style.css sheet in the directory.\n&lt; nav&gt; initializes a navigation page, with the header PIC16B Message Bank! using &lt; h1 &gt; keyword. &lt; ul&gt; initializes an unordered list, while &lt; li&gt; is each individual list entry.\n&lt; section&gt; creates a new section, while each {% block} where the content of the page will be added.\n\n\nbase.html\nWhat this code is doing is both serving as a base for additional html files, as well as formatting the base page. It lays out a page title, as well as navigation functionality to visit other pages. We use Flask’s render_template function a lot in this program to render the accompanying HTML file. the submit message page is rendered using the below function, which renders the submit.html file and inserts a message if an input is received.\nSee below for the app.py function that renders the message submission page, as well as the accompanying submit.html file.\n\n# Like before, we use @app.route. \n#However, now we must define the methods to ensure that user submission is possible by adding methods=['GET, POST']\n\n#function to submit messages to the database\n@app.route('/submit/', methods=['GET', 'POST'])\ndef submit_message():\n    #different methods for diff request methods\n    if request.method == 'GET':\n        #if 'GET' just render the template\n        return render_template('submit.html')\n    \n    else:\n        try:\n            handle, message = insert_message(request)\n            #render the template with a thank you note\n            return render_template('submit.html', thank_you = True, message = message, handle = handle)\n        except:\n            return render_template('submit.html', error=True)\n\n\n{% extends \"base.html\" %} \n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n    &lt;form action=\"/submit\" method=\"post\"&gt;\n        &lt;label for=\"message\"&gt;Message:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"message\" name=\"message\"&gt;&lt;br&gt;\n        &lt;label for=\"name\"&gt;Name:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"name\" name=\"name\"&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit\"&gt;\n    &lt;/form&gt;\n{% endblock %}\n\n\n\nHTML explanation\nWe extend base.html to adopt the style elements from style.css. We create a block titled ‘Submit Message’ to create a header for the page.\nAdditionally, we create a new block with a form element to allow the website to take user input in the form of messages and handles. I create multiple input boxes to allow the user to input both the message and their name, as well as a button to submit (which calls the submit_message function)\n\n\nMessage Submission Page\nThis page is run by the function submit_message(), which essentially receives a message from the user and renders the submit.html page. The submit.html file “extends” the base.html file, which essentially means that many components, like the style.css, is automatically transferred from the base.html file to submit.html. See below for a screenshot of this page.\n\n\n\nMessageSubmission\n\n\n\n\nDatabase Management\nIt connects to the global g object and creates a table for receiving messages with the name attached. The insert_messages function connects to that new table and inserts any messages received from the submit_messages page using a SQL query.\nSee below for the code for inserting a message into the SQL message database.\n\n@app.route('/getmsg/', methods=['GET'])\ndef get_message_db():\n\n    #try to retrieve database connection from global g object\n    try:\n        db = g.message_db\n    except AttributeError:\n        #if no database connection, connect to the database and call query to create a table called messages\n        db = g.message_db = sqlite3.connect(DATABASE)\n        db.execute('''\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY,\n                handle TEXT,\n                message TEXT\n            )\n        ''')\n    return db\n\n@app.route('/insert_msg/', methods=['POST'])\ndef insert_message(request):\n    # get message and handle from request\n    try:\n        # access the user input from the HTML webpage\n        message = request.form['message']\n        handle = request.form['name']\n\n        #connect to the SQL Database\n        db = get_message_db()\n        cursor = db.cursor()\n        #execute query to insert the user input into the databse\n        cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n        db.commit()\n\n        #return a redirection back to the /submit page of the website\n        return redirect('/submit/')\n    \n    #exception if an error occurs\n    except Exception as e:\n        return render_template('submit.html', error=True)\n\n\n\ninsert_message(request)\nWhat this function does is connect to the message database and inserts the user-inputted message into the database so that it can be displayed in the message viewing page. It connects to the database using sqlite3, and executes the query, which inserts a message into the database table.\nSee below for how the message viewing page is run.\n\n\nRandom Messages Page\n\n#function to return a random message from the db\n@app.route('/random_messages/&lt;int:n&gt;')\ndef random_message(n):\n    #connect to the SQL database\n    db = get_message_db()\n    cursor = db.cursor()\n    #run query to select a random list of messages\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    #initialize messages to be all the random messages from the above query\n    messages = cursor.fetchall()\n    #render the appropriate page with those messages\n    return render_template('view.html', messages=messages)\n\n#renders the random messages page\n@app.route('/random_messages/')\ndef render_view_template():\n    #display the first 5 messages\n    messages = random_message(5)\n\n    return render_template('view.html', messages = messages)\n\nThese functions connect to the message database, executes a query to randomly select a message, to return and render a list of previously inputted, randomly selected messages to the user.\nSee below for the view.html file.\n\n{% extends \"base.html\" %}\n\n{% block content %}\n    &lt;h2&gt;Random Messages&lt;/h2&gt;\n    &lt;ul&gt;\n        {% for message in messages %}\n            &lt;li&gt;\n                &lt;strong&gt;{{ message.handle }}&lt;/strong&gt; - {{ message.message }}\n            &lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n{% endblock %}\n\n\n\nHTML explanation\nBy extending “base.html”, we can adopt all of the style elements from base.html and style.css to all pages in the website.\nAgain we create a block with the header “Random Messages”, which displays an unordered list of message handles and messages using a for loop.\n\n\nview.html\nThis code also “extends” base.html. This html code is much simpler as it simply lists the previous messages, and doesn’t need to receive any information from the website user. See an example below.\n\n\n\nRandomMessages\n\n\n\n\nstyle.css\nWe have now seen how the website functions on the back-end with the database management and user inputs. However, this must all be presentable. As I mentioned earlier, the base.html file is accompanies by a style.css file. See below for the css code.\n\n/* Import a custom font from Google Fonts */\n@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');\n\n/* Apply the custom font to the entire body */\nbody {\n    font-family: 'Roboto', sans-serif;\n    background-color: #f0f0f0; /* Light gray background */\n    color: #333; /* Dark gray text */\n}\n\n/* Style the navigation bar */\nnav {\n    background-color: #333; /* Dark background color */\n    color: white; /* White text */\n    padding: 10px;\n}\n\n/* Style navigation links */\nnav a {\n    color: white; /* White text */\n    text-decoration: none; /* Remove underline */\n    margin-right: 10px; /* Add some margin between links */\n}\n\n/* Style the header section */\nheader {\n    background-color: #007bff; /* Blue background color */\n    color: white; /* White text */\n    padding: 20px;\n    text-align: center; /* Center align text */\n}\n\n/* Add some margin around the content */\n.content {\n    margin: 20px;\n}\n\n/* Style forms */\nform {\n    margin-bottom: 20px; /* Add margin below forms */\n}\n\n/* Style text inputs and buttons */\ninput[type=\"text\"], button {\n    padding: 10px;\n    margin-right: 10px;\n}\n\n/* Style buttons */\nbutton {\n    background-color: #007bff; /* Blue background color */\n    color: white; /* White text */\n    border: none; /* Remove border */\n    cursor: pointer; /* Change cursor to pointer */\n}\n\n/* Change button color on hover */\nbutton:hover {\n    background-color: #0056b3; /* Darker blue on hover */\n}\n\n/* Style unordered lists */\nul {\n    list-style-type: none; /* Remove bullet points */\n    padding: 0; /* Remove default padding */\n}\n\n/* Add some margin below list items */\nli {\n    margin-bottom: 10px;\n}\n\nThis code goes through many different customizations, such as font type, font size and color, page color, etc. By using certain keyword for each customization, I am able to change how the styling of the HTML will be rendered.\n\n\nConclusion\nAs we can see, there are many different things to account for in web development. Hopefully, this blog post was informational and helpful in developing your next webpage. Thank you!\n\nAndrew Han"
  },
  {
    "objectID": "posts/Palmer Penguins/index.html",
    "href": "posts/Palmer Penguins/index.html",
    "title": "Palmer Penguins Tutorial",
    "section": "",
    "text": "Blog Post URL https://andrewshan214.github.io/PIC16B/posts/Palmer%20Penguins/\n\nIntroduction\nIn this blog post, I will be giving a short tutorial on how to make a simple data visualization of the “Palmer Penguins” data set. More specifically, I will be showing a simple and informative visualization as a scatter plot to visualize the relationship between penguin culmen length and culmen depth.\nFirst, we load the necessary packages and data, as shown below.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = \"/Users/andrewhan/PIC16B/posts/Palmer Penguins/palmer_penguins.csv\"\npenguins = pd.read_csv(data)\n\n\n\nFiltering/cleaning the data\nNext, we will filter our data set to drop any entries with NaN values for their culmen length and/or depth.\nUsing that filtered data, we will group the data based on those categories. Then we will plot the data using the matplotlib package. Using key words, we can choose the type of chart we want, as well as the colors. In this case, I went with a scatter plot with blue dots.\nAfter adding the titles and labels, we can display our data visualization!\n\npenguins_filtered = penguins.dropna(subset = ['Culmen Length (mm)', 'Culmen Depth (mm)'])\n\nplt.figure(figsize=(10, 6))\nplt.scatter(penguins['Culmen Length (mm)'], penguins['Culmen Depth (mm)'], c='blue', alpha=0.7)\nplt.title('Scatter Plot of Culmen Length vs Culmen Depth')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConclusion\nAs we can see, there is scatter plot with all of our data points (penguins) plotted with their respective culmen length and depth. It seems that there is no identifiable trend between their length and depth measurements as there are many penguins with both proportionally larger culmen lengths and culmen depths."
  },
  {
    "objectID": "posts/Homework 5 - Image Classification/index.html",
    "href": "posts/Homework 5 - Image Classification/index.html",
    "title": "Image Classification Tutorial",
    "section": "",
    "text": "Introduction\nThe url for this blog is: https://andrewshan214.github.io/PIC16B/\nThe url for the github repo is: https://github.com/andrewshan214/PIC16B\nIn this blog post, I will be giving a short tutorial on how to implement an image classification model to distinguish between photos of dogs and cats.\nFirst, we import the proper packages in Python to develop our model. (See below)\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n\n\nCreating the dataset\nBelow is code that is used to create datasets for training, validation, and testing.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\nResizing the images\nBelow, we write code to resize all of the images to a standard size of (150, 150), as well as to rapidly read data by altering the batch_size to 64.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\nVisualizing the dataset\nBelow, I have implemented a function called visualize_dataset) that creates two lists, and adds 3 random images of dogs and cats in each, respectively. i use the “take” method to get images of each animal from the dataset to fill the lists.\nThen I iterate through the lists to create a plot that displays 3 random cats in the first row and 3 random dogs in the second row, using the pyplot library.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_dataset(dataset, num_samples=3, title=\"\"):\n    \n    plt.figure(figsize=(15, 6))\n\n    # initialize empty sets for each animal\n    cat_images, dog_images = [], []\n    #for loop to iterate through each image in the dataset\n    for images, labels in dataset.take(1):\n        for image, label in zip(images, labels):\n            if label == 0:\n                cat_images.append(image.numpy())\n            else:\n                dog_images.append(image.numpy())\n\n    for i in range(num_samples):\n        # Plot cat images in the first row\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(cat_images[i].astype(\"uint8\"))\n        plt.title(\"Cat\")\n        plt.axis(\"off\")\n\n        # Plot dog images in the second row\n        plt.subplot(2, num_samples, i + num_samples + 1)\n        plt.imshow(dog_images[i].astype(\"uint8\"))\n        plt.title(\"Dog\")\n        plt.axis(\"off\")\n\n    plt.show()\n\n\n\nvisualize_dataset(train_ds, title=\"Random Samples from Training Dataset\")\n\n2024-03-19 11:17:30.216798: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\n\n\n\n\n\n\n\n\nChecking Label Frequencies\nBelow, we’ve created an iterator that can go through the dataset, which we’ve used to count the total number of images of both cats and dogs by using a simple for loop.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncat_count = 0\ndog_count = 0\n\n# Iterate through labels\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    elif label == 1:\n        dog_count += 1\n\nprint(\"Number of images with label 0 (cat):\", cat_count)\nprint(\"Number of images with label 1 (dog):\", dog_count)\n\nNumber of images with label 0 (cat): 4637\nNumber of images with label 1 (dog): 4668\n\n\n\n\nCreating model1\nBelow we implemented our first model, which includes different layers to the model. Each layer processes a bit of the input data and produces an output. Each layer consists of a set of neurons that perform specific computation on the input data.\nAfter implementing the layers into model1, we compile and fit the model to evaluate the test and validation accuracy.\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\n# history = model1.fit(train_ds.batch(batch_size), \n#                       epochs=20, \n#                       validation_data=validation_ds.batch(batch_size))\n\nEpoch 1/20\n146/146 [==============================] - 306s 2s/step - loss: 14.9503 - accuracy: 0.5437 - val_loss: 0.6757 - val_accuracy: 0.5645\nEpoch 2/20\n146/146 [==============================] - 311s 2s/step - loss: 0.6935 - accuracy: 0.5439 - val_loss: 0.6892 - val_accuracy: 0.5133\nEpoch 3/20\n146/146 [==============================] - 312s 2s/step - loss: 0.6764 - accuracy: 0.5559 - val_loss: 0.6800 - val_accuracy: 0.5469\nEpoch 4/20\n146/146 [==============================] - 336s 2s/step - loss: 0.6616 - accuracy: 0.5756 - val_loss: 0.6781 - val_accuracy: 0.5503\nEpoch 5/20\n146/146 [==============================] - 316s 2s/step - loss: 0.6549 - accuracy: 0.5824 - val_loss: 0.6979 - val_accuracy: 0.5391\nEpoch 6/20\n146/146 [==============================] - 320s 2s/step - loss: 0.6307 - accuracy: 0.6098 - val_loss: 0.7220 - val_accuracy: 0.5525\nEpoch 7/20\n146/146 [==============================] - 330s 2s/step - loss: 0.6604 - accuracy: 0.6111 - val_loss: 0.7156 - val_accuracy: 0.5292\nEpoch 8/20\n146/146 [==============================] - 349s 2s/step - loss: 0.6327 - accuracy: 0.6101 - val_loss: 0.7824 - val_accuracy: 0.5297\nEpoch 9/20\n146/146 [==============================] - 360s 2s/step - loss: 0.6224 - accuracy: 0.6131 - val_loss: 0.7622 - val_accuracy: 0.5426\nEpoch 10/20\n146/146 [==============================] - 315s 2s/step - loss: 0.5934 - accuracy: 0.6328 - val_loss: 0.7487 - val_accuracy: 0.5404\nEpoch 11/20\n146/146 [==============================] - 327s 2s/step - loss: 0.6011 - accuracy: 0.6374 - val_loss: 0.8474 - val_accuracy: 0.5361\nEpoch 12/20\n146/146 [==============================] - 363s 2s/step - loss: 0.5691 - accuracy: 0.6603 - val_loss: 0.8148 - val_accuracy: 0.5292\nEpoch 13/20\n146/146 [==============================] - 317s 2s/step - loss: 0.5562 - accuracy: 0.6709 - val_loss: 0.8807 - val_accuracy: 0.5464\nEpoch 14/20\n146/146 [==============================] - 295s 2s/step - loss: 0.5392 - accuracy: 0.6847 - val_loss: 1.0191 - val_accuracy: 0.5464\nEpoch 15/20\n146/146 [==============================] - 313s 2s/step - loss: 0.4992 - accuracy: 0.7097 - val_loss: 1.3356 - val_accuracy: 0.5439\nEpoch 16/20\n146/146 [==============================] - 315s 2s/step - loss: 0.4804 - accuracy: 0.7262 - val_loss: 1.1899 - val_accuracy: 0.5610\nEpoch 17/20\n146/146 [==============================] - 318s 2s/step - loss: 0.4991 - accuracy: 0.7160 - val_loss: 1.0055 - val_accuracy: 0.5890\nEpoch 18/20\n146/146 [==============================] - 324s 2s/step - loss: 0.4654 - accuracy: 0.7424 - val_loss: 1.1510 - val_accuracy: 0.5761\nEpoch 19/20\n146/146 [==============================] - 316s 2s/step - loss: 0.4244 - accuracy: 0.7639 - val_loss: 1.0391 - val_accuracy: 0.5739\nEpoch 20/20\n146/146 [==============================] - 298s 2s/step - loss: 0.4085 - accuracy: 0.7900 - val_loss: 1.1309 - val_accuracy: 0.5641\n\n\nSome things I experimented with was increasing the model complexity by adding more layers. Additionally, I tried to regularized the data to prevent overfitting.\nThe validation accuracy of my model stabilized between 56% and 57% during training, which is higher than the baseline.\n\n\nVisualizing the model accuracy\nWe use matplotlib.pyplot to visualize the training and validation history, as seen below:\n\nfrom matplotlib import pyplot as plt\n# plotting the training accuracy\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nModel with Data Augmentation\nBy augmenting the data, it allows the model to recognize images, even if they’re rotated or mirrored.\nThe random flip layer is implemented, by randomly mirroring images, and initializing the layer to that.\nSimilarly, the random rotation layer by randomly rotating the images, and training a layer to the random rotations.\nI’ve added before and after photos for the effect of both layers for visual aid. See code and images below.\n\n#random_flip_layer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Load a few sample images\nsample_images = []\nfor image, label in train_ds.take(1):\n    sample_images = image.numpy()[:4]  # Take the first three images\n\n# Normalize pixel values to the range [0, 1]\nsample_images = sample_images.astype(\"float32\") / 255.0\n\n# Create a RandomFlip layer\nrandom_flip_layer = tf.keras.layers.RandomFlip(\"horizontal\")\n\n# Apply RandomFlip to the sample images\naugmented_images = random_flip_layer(sample_images)\n\n# Plot original and augmented images\nplt.figure(figsize=(10, 7))\nfor i in range(0, 3):\n    # Original image\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(sample_images[i])\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    # Augmented image\n    plt.subplot(2, 3, i + 4)\n    plt.imshow(augmented_images[i])\n    plt.title(\"Augmented\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n#RandomRotation layer\n\nsample_images = []\nfor image, label in train_ds.take(1):\n    sample_images = image.numpy()[:3]  # Take the first three images\n\n# Normalize pixel values to the range [0, 1]\nsample_images = sample_images.astype(\"float32\") / 255.0\n\n# Create a RandomRotation layer\nrandom_rotation_layer = tf.keras.layers.RandomRotation(factor=0.5)\n\n# Apply RandomRotation to the sample images\naugmented_images = random_rotation_layer(sample_images)\n\n# Plot original and augmented images\nplt.figure(figsize=(10, 7))\nfor i in range(0, 3):\n    # Original image\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(sample_images[i])\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    # Augmented image\n    plt.subplot(2, 3, i + 4)\n    plt.imshow(augmented_images[i])\n    plt.title(\"Augmented\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel 2\nAfter creating those two layers above, we include them in the second model (in addition to all the other layers from model1) to hopefully find a more accurate model. See code below.\n\n#creating model2 with augmentation layers\n\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(factor=0.2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile model2\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train model2\nhistory2 = model2.fit(train_ds, \n                      epochs=20, \n                      validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 55s 370ms/step - loss: 22.8603 - accuracy: 0.5318 - val_loss: 0.6931 - val_accuracy: 0.5193\nEpoch 2/20\n146/146 [==============================] - 55s 377ms/step - loss: 0.6906 - accuracy: 0.5332 - val_loss: 0.6912 - val_accuracy: 0.5335\nEpoch 3/20\n146/146 [==============================] - 58s 398ms/step - loss: 0.6921 - accuracy: 0.5378 - val_loss: 0.6896 - val_accuracy: 0.5370\nEpoch 4/20\n146/146 [==============================] - 58s 399ms/step - loss: 0.6914 - accuracy: 0.5396 - val_loss: 0.6888 - val_accuracy: 0.5344\nEpoch 5/20\n146/146 [==============================] - 60s 413ms/step - loss: 0.6912 - accuracy: 0.5410 - val_loss: 0.6853 - val_accuracy: 0.5516\nEpoch 6/20\n146/146 [==============================] - 63s 430ms/step - loss: 0.6902 - accuracy: 0.5405 - val_loss: 0.6860 - val_accuracy: 0.5585\nEpoch 7/20\n146/146 [==============================] - 98s 674ms/step - loss: 0.6850 - accuracy: 0.5566 - val_loss: 0.6872 - val_accuracy: 0.5365\nEpoch 8/20\n146/146 [==============================] - 107s 735ms/step - loss: 0.6743 - accuracy: 0.5927 - val_loss: 0.6691 - val_accuracy: 0.5929\nEpoch 9/20\n146/146 [==============================] - 83s 564ms/step - loss: 0.6794 - accuracy: 0.5552 - val_loss: 0.6819 - val_accuracy: 0.5787\nEpoch 10/20\n146/146 [==============================] - 78s 536ms/step - loss: 0.6782 - accuracy: 0.5668 - val_loss: 0.6802 - val_accuracy: 0.5709\nEpoch 11/20\n146/146 [==============================] - 94s 639ms/step - loss: 0.6621 - accuracy: 0.6145 - val_loss: 0.6571 - val_accuracy: 0.6195\nEpoch 12/20\n146/146 [==============================] - 84s 575ms/step - loss: 0.6636 - accuracy: 0.6098 - val_loss: 0.6543 - val_accuracy: 0.6165\nEpoch 13/20\n146/146 [==============================] - 72s 493ms/step - loss: 0.6567 - accuracy: 0.6176 - val_loss: 0.6504 - val_accuracy: 0.6242\nEpoch 14/20\n146/146 [==============================] - 75s 511ms/step - loss: 0.6474 - accuracy: 0.6352 - val_loss: 0.6377 - val_accuracy: 0.6432\nEpoch 15/20\n146/146 [==============================] - 69s 468ms/step - loss: 0.6507 - accuracy: 0.6311 - val_loss: 0.6370 - val_accuracy: 0.6466\nEpoch 16/20\n146/146 [==============================] - 81s 553ms/step - loss: 0.6461 - accuracy: 0.6357 - val_loss: 0.6307 - val_accuracy: 0.6496\nEpoch 17/20\n146/146 [==============================] - 73s 497ms/step - loss: 0.6444 - accuracy: 0.6355 - val_loss: 0.6438 - val_accuracy: 0.6281\nEpoch 18/20\n146/146 [==============================] - 70s 478ms/step - loss: 0.6401 - accuracy: 0.6403 - val_loss: 0.6325 - val_accuracy: 0.6561\nEpoch 19/20\n146/146 [==============================] - 69s 474ms/step - loss: 0.6352 - accuracy: 0.6479 - val_loss: 0.6508 - val_accuracy: 0.6320\nEpoch 20/20\n146/146 [==============================] - 83s 568ms/step - loss: 0.6357 - accuracy: 0.6444 - val_loss: 0.6530 - val_accuracy: 0.6208\n\n\nThe accuracy of my model stabilized between 62% and 63% during training, which is a bit higher than in model1, and still higher than 60%.\nIn model2, if the accuracy stabilized around 64% to 65% during training and the validation accuracy closely tracked the training accuracy without diverging significantly, it indicates that the model may not be overfitting. Signs of overfitting include a large gap between training and validation accuracy or when the validation accuracy starts decreasing while the training accuracy continues to increase.\nAs with model1, we will visualize our results.\n\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nModel 3\nWe implemented model 3 hoping to make training the model faster by implemented a preprocessor layer that would handle the scaling of the RGB values prior to the training process. This allows the computer to focus more energy on handling actual signal in the data and less energy having the weights adjust to the data scale.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = [i], outputs = [x])\n\nmodel3 = tf.keras.Sequential([\n    preprocessor,\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(factor=0.2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile model3\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\nhistory3 = model3.fit(train_ds, \n                      epochs=20,  # Increase the number of epochs\n                      validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 67s 452ms/step - loss: 0.7907 - accuracy: 0.5278 - val_loss: 0.6691 - val_accuracy: 0.6032\nEpoch 2/20\n146/146 [==============================] - 67s 455ms/step - loss: 0.6476 - accuracy: 0.6163 - val_loss: 0.6015 - val_accuracy: 0.6917\nEpoch 3/20\n146/146 [==============================] - 72s 496ms/step - loss: 0.6210 - accuracy: 0.6577 - val_loss: 0.5661 - val_accuracy: 0.7077\nEpoch 4/20\n146/146 [==============================] - 66s 449ms/step - loss: 0.5879 - accuracy: 0.6857 - val_loss: 0.5442 - val_accuracy: 0.7197\nEpoch 5/20\n146/146 [==============================] - 72s 495ms/step - loss: 0.5655 - accuracy: 0.7035 - val_loss: 0.5226 - val_accuracy: 0.7360\nEpoch 6/20\n146/146 [==============================] - 74s 509ms/step - loss: 0.5481 - accuracy: 0.7218 - val_loss: 0.5098 - val_accuracy: 0.7451\nEpoch 7/20\n146/146 [==============================] - 64s 440ms/step - loss: 0.5369 - accuracy: 0.7279 - val_loss: 0.5018 - val_accuracy: 0.7515\nEpoch 8/20\n146/146 [==============================] - 67s 460ms/step - loss: 0.5211 - accuracy: 0.7411 - val_loss: 0.4842 - val_accuracy: 0.7696\nEpoch 9/20\n146/146 [==============================] - 74s 505ms/step - loss: 0.5116 - accuracy: 0.7488 - val_loss: 0.4798 - val_accuracy: 0.7786\nEpoch 10/20\n146/146 [==============================] - 80s 544ms/step - loss: 0.4998 - accuracy: 0.7515 - val_loss: 0.4784 - val_accuracy: 0.7777\nEpoch 11/20\n146/146 [==============================] - 80s 545ms/step - loss: 0.4951 - accuracy: 0.7591 - val_loss: 0.4841 - val_accuracy: 0.7739\nEpoch 12/20\n146/146 [==============================] - 79s 539ms/step - loss: 0.4900 - accuracy: 0.7635 - val_loss: 0.4783 - val_accuracy: 0.7726\nEpoch 13/20\n146/146 [==============================] - 80s 545ms/step - loss: 0.4768 - accuracy: 0.7709 - val_loss: 0.4768 - val_accuracy: 0.7696\nEpoch 14/20\n146/146 [==============================] - 74s 507ms/step - loss: 0.4806 - accuracy: 0.7703 - val_loss: 0.4698 - val_accuracy: 0.7743\nEpoch 15/20\n146/146 [==============================] - 67s 456ms/step - loss: 0.4709 - accuracy: 0.7769 - val_loss: 0.4701 - val_accuracy: 0.7799\nEpoch 16/20\n146/146 [==============================] - 68s 465ms/step - loss: 0.4588 - accuracy: 0.7836 - val_loss: 0.4648 - val_accuracy: 0.7902\nEpoch 17/20\n146/146 [==============================] - 67s 460ms/step - loss: 0.4580 - accuracy: 0.7819 - val_loss: 0.4735 - val_accuracy: 0.7915\nEpoch 18/20\n146/146 [==============================] - 70s 481ms/step - loss: 0.4575 - accuracy: 0.7838 - val_loss: 0.4757 - val_accuracy: 0.7898\nEpoch 19/20\n146/146 [==============================] - 68s 467ms/step - loss: 0.4505 - accuracy: 0.7886 - val_loss: 0.4665 - val_accuracy: 0.7889\nEpoch 20/20\n146/146 [==============================] - 74s 504ms/step - loss: 0.4492 - accuracy: 0.7844 - val_loss: 0.4759 - val_accuracy: 0.7906\n\n\nThe validation accuracy stabilized between 80% and 82% during training This is much higher than the val_accuracy of model1, which was ~60%.\nSince there is not a significant gap between the training and validation accuracy, it suggests that model3 is not overfitting the data.\n\n\nVisualizing model3 results\nWe will once again visualize these results.\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nTransfer Learning\nTransfer learning is essentially accessing a pre-existing base model and incorporating it into our own model. The first few lines of the code below is downloading MobileNetV3Large, which will serve as our base model. We create a base_model_layer that we implement into our model4, which is constructed in the same manner as the previous 3 models.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights=None)\nweights_path = '/Users/andrewhan/PIC16B/posts/Homework 5 - Image Classification/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5'\nbase_model.load_weights(weights_path)\n\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(factor=0.2)\n])\n\nmodel4 = tf.keras.Sequential([\n    data_augmentation,\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(2, activation='softmax')  \n])\n\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\nhistory4 = model4.fit(train_ds, \n                      epochs=20,  # Increase the number of epochs\n                      validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 52s 336ms/step - loss: 0.2513 - accuracy: 0.8903 - val_loss: 0.0946 - val_accuracy: 0.9669\nEpoch 2/20\n146/146 [==============================] - 635s 4s/step - loss: 0.1390 - accuracy: 0.9422 - val_loss: 0.0785 - val_accuracy: 0.9729\nEpoch 3/20\n146/146 [==============================] - 425s 3s/step - loss: 0.1180 - accuracy: 0.9521 - val_loss: 0.0724 - val_accuracy: 0.9746\nEpoch 4/20\n146/146 [==============================] - 39s 269ms/step - loss: 0.1152 - accuracy: 0.9517 - val_loss: 0.0735 - val_accuracy: 0.9742\nEpoch 5/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.1124 - accuracy: 0.9535 - val_loss: 0.0663 - val_accuracy: 0.9759\nEpoch 6/20\n146/146 [==============================] - 37s 256ms/step - loss: 0.1105 - accuracy: 0.9557 - val_loss: 0.0665 - val_accuracy: 0.9785\nEpoch 7/20\n146/146 [==============================] - 37s 256ms/step - loss: 0.1046 - accuracy: 0.9569 - val_loss: 0.0719 - val_accuracy: 0.9742\nEpoch 8/20\n146/146 [==============================] - 37s 256ms/step - loss: 0.1065 - accuracy: 0.9560 - val_loss: 0.0656 - val_accuracy: 0.9764\nEpoch 9/20\n146/146 [==============================] - 38s 263ms/step - loss: 0.1007 - accuracy: 0.9557 - val_loss: 0.0631 - val_accuracy: 0.9789\nEpoch 10/20\n146/146 [==============================] - 39s 268ms/step - loss: 0.0992 - accuracy: 0.9584 - val_loss: 0.0697 - val_accuracy: 0.9742\nEpoch 11/20\n146/146 [==============================] - 39s 266ms/step - loss: 0.0962 - accuracy: 0.9597 - val_loss: 0.0655 - val_accuracy: 0.9781\nEpoch 12/20\n146/146 [==============================] - 40s 273ms/step - loss: 0.0930 - accuracy: 0.9617 - val_loss: 0.0640 - val_accuracy: 0.9776\nEpoch 13/20\n146/146 [==============================] - 38s 261ms/step - loss: 0.0917 - accuracy: 0.9632 - val_loss: 0.0628 - val_accuracy: 0.9785\nEpoch 14/20\n146/146 [==============================] - 39s 266ms/step - loss: 0.0908 - accuracy: 0.9613 - val_loss: 0.0648 - val_accuracy: 0.9781\nEpoch 15/20\n146/146 [==============================] - 39s 268ms/step - loss: 0.0892 - accuracy: 0.9638 - val_loss: 0.0632 - val_accuracy: 0.9802\nEpoch 16/20\n146/146 [==============================] - 41s 279ms/step - loss: 0.0940 - accuracy: 0.9617 - val_loss: 0.0658 - val_accuracy: 0.9781\nEpoch 17/20\n146/146 [==============================] - 39s 266ms/step - loss: 0.0881 - accuracy: 0.9639 - val_loss: 0.0687 - val_accuracy: 0.9772\nEpoch 18/20\n146/146 [==============================] - 40s 273ms/step - loss: 0.0911 - accuracy: 0.9636 - val_loss: 0.0671 - val_accuracy: 0.9776\nEpoch 19/20\n146/146 [==============================] - 39s 268ms/step - loss: 0.0965 - accuracy: 0.9613 - val_loss: 0.0684 - val_accuracy: 0.9768\nEpoch 20/20\n146/146 [==============================] - 39s 268ms/step - loss: 0.0931 - accuracy: 0.9609 - val_loss: 0.0657 - val_accuracy: 0.9764\n\n\nThe model4 validation accuracy stabilzed between 97% and 98%, which is greater than the required 93% This val_accuracy is also much higher than from model1, which was around 60%.\nThere does not seem to be any overfitting as there is not much difference between the test and validation accuracy.\n\n\nVisualizing the results\n\nplt.plot(history4.history[\"accuracy\"], label = \"training\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nScore on Test Data\nThis code below will evaluate the accuracy of model4, which is the most performant model of the four. This will evaluate the unseen test dataset and print the value it returns.\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\n\n# Print the test accuracy\nprint(f'Test accuracy: {test_accuracy}')\n\n37/37 [==============================] - 8s 198ms/step - loss: 0.0741 - accuracy: 0.9708\nTest accuracy: 0.970765233039856\n\n\nThe test accuracy was over 97%, so we can see that the model works well. This suggests that the model has effectively learned to distinguish between cats and dogs in the test dataset.\n\n\nConclusion\nIn conclusion, we can see that adding more layers (or more information for the model to account for), the more accurate the model will be at recognizing dogs and cats. As we added more layers to our models, we received higher marks for the validation accuracy in each model."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello classmates! I’m a third year Mathematics/Economics Major, with a specialization in Computing.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Final Project Submission/index.html",
    "href": "posts/Final Project Submission/index.html",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "Andrew Han, Jessica Xiao, Justine Constantino\n\n\nThe purpose of our project is to analyze user music listening habits, specifically artists and top songs, and match them to a country based on similarities in song and artist taste. The target audience for our project is those who are interested in seeking out songs from different countries that they may have similar interests in as well as learning what cultures have similar music tastes as the user. To accomplish this, we scraped data from Spotify charts, implemented a SQL database, and finally displayed data visualizations via a Plotly Dash website.\nGithub Repo link: https://github.com/jbean402/UserVsTheWorld/tree/main\nBlog post link: https://andrewshan214.github.io/PIC16B/\n\n\n\nFirst, we imported the necessary libraries for web scraping. We used BeatifulSoup to scrape our data.\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport requests\nThe code scrapes URLs of weekly Spotify charts from the specified webpage and stores them in the link_list variable.\nsoup = BeautifulSoup(requests.get('https://kworb.net/spotify/').text)\nlink_list = [l['href'] for l in soup.find('table').find_all('a', href=True) \n if ('weekly' in l['href']) and ('totals' not in l['href'])]\n\nlink_list[1:]\n['country/us_weekly.html',\n 'country/gb_weekly.html',\n 'country/ad_weekly.html',\n 'country/ar_weekly.html',\n 'country/au_weekly.html',\n 'country/at_weekly.html',\n 'country/by_weekly.html',\n 'country/be_weekly.html',\n 'country/bo_weekly.html',\n 'country/br_weekly.html',\n 'country/bg_weekly.html',\n 'country/ca_weekly.html',\n 'country/cl_weekly.html',\n 'country/co_weekly.html',\n 'country/cr_weekly.html',\n 'country/cy_weekly.html',\n 'country/cz_weekly.html',\n 'country/dk_weekly.html',\n 'country/do_weekly.html',\n 'country/ec_weekly.html',\n 'country/eg_weekly.html',\n 'country/sv_weekly.html',\n 'country/ee_weekly.html',\n 'country/fi_weekly.html',\n 'country/fr_weekly.html',\n 'country/de_weekly.html',\n 'country/gr_weekly.html',\n 'country/gt_weekly.html',\n 'country/hn_weekly.html',\n 'country/hk_weekly.html',\n 'country/hu_weekly.html',\n 'country/is_weekly.html',\n 'country/in_weekly.html',\n 'country/id_weekly.html',\n 'country/ie_weekly.html',\n 'country/il_weekly.html',\n 'country/it_weekly.html',\n 'country/jp_weekly.html',\n 'country/kz_weekly.html',\n 'country/lv_weekly.html',\n 'country/lt_weekly.html',\n 'country/lu_weekly.html',\n 'country/my_weekly.html',\n 'country/mt_weekly.html',\n 'country/mx_weekly.html',\n 'country/ma_weekly.html',\n 'country/nl_weekly.html',\n 'country/nz_weekly.html',\n 'country/ni_weekly.html',\n 'country/ng_weekly.html',\n 'country/no_weekly.html',\n 'country/pk_weekly.html',\n 'country/pa_weekly.html',\n 'country/py_weekly.html',\n 'country/pe_weekly.html',\n 'country/ph_weekly.html',\n 'country/pl_weekly.html',\n 'country/pt_weekly.html',\n 'country/ro_weekly.html',\n 'country/ru_weekly.html',\n 'country/sa_weekly.html',\n 'country/sg_weekly.html',\n 'country/sk_weekly.html',\n 'country/za_weekly.html',\n 'country/kr_weekly.html',\n 'country/es_weekly.html',\n 'country/se_weekly.html',\n 'country/ch_weekly.html',\n 'country/tw_weekly.html',\n 'country/th_weekly.html',\n 'country/tr_weekly.html',\n 'country/ua_weekly.html',\n 'country/ae_weekly.html',\n 'country/uy_weekly.html',\n 'country/ve_weekly.html',\n 'country/vn_weekly.html']\n\n\nWe cleaned the data by iterating over a link_list obtained from scraping Spotify weekly charts from the website ‘https://kworb.net/spotify/’. We extracted and processed Spotify weekly chart data from the specified URLs, splitting artist and song title information and dropping unnecessary columns and displayed the resulting data frame.\nfor link in link_list[1:3]:\n    df = pd.read_html(f'https://kworb.net/spotify/{link}')[0]\n\n    if 'Artist and Title' in df.columns: \n        df[['artist_name', 'song_title']] = df['Artist and Title'].str.split('-', n=1, expand = True) \n        df.drop(columns=['Artist and Title'], inplace=True)\n        df.drop(columns=['P+', 'Wks', 'Pk', '(x?)', 'Streams', 'Streams+', 'Total'], inplace = True) \n\n    display(df) \n\n\n\noutput\n\n\nNext, we extracted country names from specific tags in the HTML content parsed using BeautifulSoup. It filters out certain country names specified in the filters list and then prints the remaining country names.\ntd_tags = soup.find_all('td', class_=\"mp text\")\nfilters = ['Global'] \ncountry_names_list = []\n\nfor i in range(0, len(td_tags), 2):\n    country_name = td_tags[i].get_text()\n    \n    if country_name not in filters:\n        country_names_list.append(country_name) \n\nprint(country_names_list)\n['United States', 'United Kingdom', 'Andorra', 'Argentina', 'Australia', 'Austria', 'Belarus', 'Belgium', 'Bolivia', 'Brazil', 'Bulgaria', 'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Cyprus', 'Czech Republic', 'Denmark', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Guatemala', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Malta', 'Mexico', 'Morocco', 'Netherlands', 'New Zealand', 'Nicaragua', 'Nigeria', 'Norway', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Russia', 'Saudi Arabia', 'Singapore', 'Slovakia', 'South Africa', 'South Korea', 'Spain', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Turkey', 'Ukraine', 'United Arab Emirates', 'Uruguay', 'Venezuela', 'Vietnam']\nFinally, we created a combined dataframe with the columns Pos, artist_name, song_title, and country. We saved the dataframe to a csv file.\ncombined_df = pd.DataFrame()\n\nfor link, country_name in zip(link_list[1:], country_names_list):\n    df = pd.read_html(f'https://kworb.net/spotify/{link}')[0]\n\n    if 'Artist and Title' in df.columns: \n        df[['artist_name', 'song_title']] = df['Artist and Title'].str.split('-', n=1, expand = True) \n        df.drop(columns=['Artist and Title', 'P+', 'Wks', 'Pk', '(x?)', 'Streams', 'Streams+', 'Total'], inplace = True) \n        \n    df['country'] = country_name\n    \n    combined_df = combined_df.append(df, ignore_index = True) \n\ncombined_df.to_csv('country_charts.csv', index=False)\n\n\n\n\nFirst, we imported the necessary libraries to create our database.\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nNext, we created a SQL Database that is organized by country name. To do this, we read the country_charts.csv file through pandas, then convert the dataframe to a SQLite database. We also defined a class DatabaseHandler with static methods for creating a SQLite database, creating a table within that database, and inserting data into the table.\ndf = pd.read_csv('/Users/andrewhan/Desktop/2023-2024/Winter_24/PIC_Class/Project/country_charts.csv')\ndf.head()\n\nconn = sqlite3.connect('country_data.db')\n\ndf.to_sql('country_data', conn, if_exists='replace', index=False)\nThe code below defines a class DatabaseHandler with static methods for creating a SQLite database, creating a table within that database, and inserting data into the table. It iterates through the CSV that we read as a dataframe and inserts the data into the db using the query seen below.\nclass DatabaseHandler:\n    @staticmethod\n    def create_database():\n        conn = sqlite3.connect('country_data.db') #creates and connects to country_data.db database\n        conn.close()\n    \n\n    @staticmethod\n    def create_table():\n        conn = sqlite3.connect('country_data.db')\n        c = conn.cursor()\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS countries (\n            country TEXT,\n            song_name TEXT,\n            artist_name TEXT,\n            rank INTEGER\n                )\n        ''')\n        conn.commit()\n        conn.close()\n\n# MULTI INDEX SQL\n    @staticmethod\n    def insert_data():\n        DatabaseHandler.create_database()\n        DatabaseHandler.create_table()\n        conn = sqlite3.connect('country_data.db')\n        c = conn.cursor()\n        c.execute('''\n            INSERT INTO countries (country, song_name, artist_name, rank)\n            SELECT country, song_name, artist_name, rank\n            FROM countries\n            WHERE (country, rank) IN (\n            SELECT country, MAX(rank)\n            FROM countries\n            GROUP BY country\n            )\n        ''')\n        conn.commit()\n        \n        #fetch data from the countries table\n        c.execute('''\n            SELECT c.country, c.song_name, c.artist_name, c.rank\n            FROM countries c\n            JOIN countries yt ON c.country = yt.country AND c.song_name = yt.song_name AND c.artist_name = yt.artist_name\n            ORDER BY c.country\n        ''')\n\n        conn.commit()\n        conn.close()\n\nDatabaseHandler.insert_data()\n\n\nTo authorize Spotify, we imported the spotipy library.\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\nNext, we defined the functions the collect user listening data and recommend countries based on the user’s top songs.\ndef collect_user_listening_data():\n    # Initialize Spotipy with your client ID, client secret, and redirect URI\n    sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id='your_client_id',\n                                                   client_secret='your_client_secret',\n                                                   redirect_uri='your_redirect_uri',\n                                                   scope='user-library-read,user-top-read'))\n\n    # Fetch the user's top artists and tracks\n    top_artists = sp.current_user_top_artists(limit=15)\n    top_tracks = sp.current_user_top_tracks(limit=200)\n\n    # Process the fetched data as needed\n    user_listening_data = {\n        'top_artists': [artist['name'] for artist in top_artists['items']],\n        'top_tracks': [track['name'] for track in top_tracks['items']]\n    }\n    return user_listening_data\n\ndef recommend_countries(user_top_songs):\n    conn = sqlite3.connect('country_data.db')\n    c = conn.cursor()\n\n    country_counts = {}\n\n    for song in user_top_songs:\n        # Query the database to retrieve associated countries\n        c.execute(\"SELECT DISTINCT country FROM countries WHERE song_name = ?\", (song,))\n        countries = c.fetchall()\n        \n        # Increment counts for each country\n        for country in countries:\n            country_name = country[0]\n            country_counts[country_name] = country_counts.get(country_name, 0) + 1\n    \n    # Close connection\n    conn.close()\n    \n    # Recommend the country with the highest count\n    top_countries = sorted(country_counts, key=country_counts.get, reverse=True)[:3]\n\n    total_top_country_occurrences = sum(country_counts[country] for country in top_countries)\n    \n    # Calculate the country score as a percentage of how much the user's top songs match the top countries\n    country_scores = {country: (country_counts[country] / total_top_country_occurrences) * 100 for country in top_countries}\n    \n    return top_countries, country_scores\n\n\n\n\nThe recommend_countries function works by comparing the user’s top songs with the top songs of each individual country. By utilizing the SQLite query, we were able to make the algorithm relatively efficient (compared to iterating through every single user’s songs and each country’s songs). It finds the “DISTINCT” songs from each country, and creates a score based off the total number of distinct song occurrences in the user’s top songs. \n\n\n\nFirst, we import the necessary libraries for visualizing the data.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport csv\n\n# data visualization \nimport plotly.express as px \n\n# Import your functions or ensure they are defined in this script\n# from your_module import collect_user_listening_data, recommend_countries\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\n\nimport dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nWe created three functions to visualize the data. plot_top_songs takes in the user’s top songs and generates a horizontal bar plot using Plotly Express. Similar to the previous function, plot_top_artists takes a list of user’s top artists as input and generates a horizontal bar plot using Plotly Express. plot_recommended_countries takes a dictionary of country scores as input and generates a scatter plot using Plotly Express.\n# ===== Visualization 1: User's Top Tracks ===== (works)\ndef plot_top_songs(user_top_songs): \n    \"\"\"\n    This plot shows the top 50 songs for the user\n    \"\"\"\n\n    # putting the list of songs into another variable  \n    songs = user_top_songs\n\n    # creating a dataframe \n    top_songs_data = pd.DataFrame()\n\n    # processing and concatenating to clean up the dataframe \n    top_songs_data['Songs'] = songs\n\n    # Adding position values to prepare for the grapg \n    top_songs_data['Position'] = top_songs_data.index + 1\n\n    # creating the plot \n    top_songs_data = top_songs_data[::-1]\n    fig = px.bar(top_songs_data, x = 'Position', y = 'Songs', orientation='h', width=1500, height=1000)\n    fig.update_layout(\n        margin=dict(l=50, r=50, t=50, b=50),  # Adjust margins as needed\n    )\n\n    return fig\n\n# ===== Visualization 2: User's Top Artists ====== (works)\ndef plot_top_artists(user_top_artists):\n    \"\"\"\n    This plot shows the top 15 artists for the user\n    \"\"\"\n\n    # putting the list of songs into another variable  \n    artists = user_top_artists\n\n    # creating a dataframe \n    top_artists_data = pd.DataFrame()\n\n    # processing and concatenating to clean up the dataframe \n    top_artists_data['Artists'] = artists\n\n    # Adding position values to prepare for the grapg \n    top_artists_data['Position'] = top_artists_data.index + 1\n\n    # creating the plot \n    # top_artists_data = top_artists_data[::-1]\n    fig = px.bar(top_artists_data, x = 'Position', y = 'Artists', orientation='h', width=1500, height=700, color='Artists')\n    fig.update_layout(\n        margin=dict(l=15, r=15, t=15, b=15),\n\n    )\n    \n    return fig\n\n# ===== Visualization 3: Recommended Countries Plot ====== (works)\ndef plot_recommended_countries(country_scores): \n    # first processing into a dataframe for the plot \n    # creating new columns, preparing for df \n    column_names = ['Country', 'Common_Songs']\n    \n    # creating a new dataframe \n    recommend_data = pd.DataFrame(list(country_scores.items()), columns=column_names)\n\n\n    #feeding into new dataframe\n    # assuming that country_scores is a list of country names \n    # countries = country_scores \n    fig = px.scatter(recommend_data, x=\"Country\", y=\"Common_Songs\")\n    fig.show()\n\n    return fig \n\n\n\nWe created a Plotly Dash web application that allows users to visualize different aspects of their music preferences. The app consists of a dropdown menu to select the type of data to visualize (top songs, top artists, or recommended countries) and a graph area where the selected data will be displayed.\n\n\n\noutput\n\n\nimport plotly.express as px \n\n# Import your functions or ensure they are defined in this script\n# from your_module import collect_user_listening_data, recommend_countries\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\n\nimport dash\nfrom dash import dcc, html\nimport dash_ag_grid as dag\nfrom dash.dependencies import Input, Output\n\n# from visualization \nfrom visualization import plot_top_songs, plot_top_artists, plot_recommended_countries\n\n# ==== PLOTLY APP ====\n\n# Define the Plotly Dash app\napp = dash.Dash(__name__)\n\n# Define the layout of the app\napp.layout = html.Div(\n    style={'display': 'flex', 'justifyContent': 'center', 'alignItems': 'center', 'flexDirection': 'column', 'backgroundColor': '#1db954'},\n    children=[\n    html.H1(children='User Vs. The World', style={'textAlign':'center', 'fontSize':50, 'fontFamily': 'Gill Sans', 'color': 'White', 'marginBottom': '10px', 'fontWeight': 'medium'}),\n    html.P(\"What country do your music tastes align with?\", style={'fontFamily': 'Gill Sans', 'fontSize': 20, 'color':'191414', 'marginTop':'10px'}),\n    dcc.Dropdown(\n        id='data-type-dropdown',\n        options=[\n            {'label': 'Top Songs', 'value': 'top_songs'},\n            {'label': 'Top Artists', 'value': 'top_artists'},\n            {'label': 'Similar Countries', 'value': 'recommended_countries'},\n        ],\n        value='top_songs',  # Default value\n        clearable=False,\n        style={'width': '300px','margin': 'auto', 'fontSize': '20px', 'fontFamily': 'Gill Sans'},\n    ),\n    dcc.Graph(id='data-plot', style={'margin': 'center', 'marginTop': '20px'})\n    ]\n)\n\n# Define a callback to update the plot based on the dropdown selection\n@app.callback(\n    Output('data-plot', 'figure'),\n    [Input('data-type-dropdown', 'value')]\n)\n\ndef update_data_plot(data_type):\n    if data_type == 'top_songs':\n        # Call function to collect user's top songs\n        user_top_songs = collect_user_top_tracks()  # Assuming you have a function like this\n        \n        # Call function to plot top songs\n        return plot_top_songs(user_top_songs)\n    \n    elif data_type == 'top_artists':\n        # Call function to collect user's top artists\n        user_top_artists = collect_user_top_artists()  # Assuming you have a function like this\n        \n        # Call function to plot top artists\n        return plot_top_artists(user_top_artists)\n    \n    elif data_type == 'recommended_countries':\n        user_top_songs = collect_user_top_tracks()\n        recommended = recommend_countries(user_top_songs)\n\n        return plot_recommended_countries(recommended)\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\nTo summarize, our project correlates a Spotify user’s listening data with a specific country. We collected data from global charts through web scraping, utilized a SQL database for data management, and showcased the insights through visualizations on a Plotly Dash website. While our project is generally free from ethical or environmental concerns, it’s essential to acknowledge potential privacy implications related to sharing user data through Spotify authorization."
  },
  {
    "objectID": "posts/Final Project Submission/index.html#project-overview",
    "href": "posts/Final Project Submission/index.html#project-overview",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "The purpose of our project is to analyze user music listening habits, specifically artists and top songs, and match them to a country based on similarities in song and artist taste. The target audience for our project is those who are interested in seeking out songs from different countries that they may have similar interests in as well as learning what cultures have similar music tastes as the user. To accomplish this, we scraped data from Spotify charts, implemented a SQL database, and finally displayed data visualizations via a Plotly Dash website.\nGithub Repo link: https://github.com/jbean402/UserVsTheWorld/tree/main\nBlog post link: https://andrewshan214.github.io/PIC16B/"
  },
  {
    "objectID": "posts/Final Project Submission/index.html#web-scraping-with-beautiful-soup",
    "href": "posts/Final Project Submission/index.html#web-scraping-with-beautiful-soup",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "First, we imported the necessary libraries for web scraping. We used BeatifulSoup to scrape our data.\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport requests\nThe code scrapes URLs of weekly Spotify charts from the specified webpage and stores them in the link_list variable.\nsoup = BeautifulSoup(requests.get('https://kworb.net/spotify/').text)\nlink_list = [l['href'] for l in soup.find('table').find_all('a', href=True) \n if ('weekly' in l['href']) and ('totals' not in l['href'])]\n\nlink_list[1:]\n['country/us_weekly.html',\n 'country/gb_weekly.html',\n 'country/ad_weekly.html',\n 'country/ar_weekly.html',\n 'country/au_weekly.html',\n 'country/at_weekly.html',\n 'country/by_weekly.html',\n 'country/be_weekly.html',\n 'country/bo_weekly.html',\n 'country/br_weekly.html',\n 'country/bg_weekly.html',\n 'country/ca_weekly.html',\n 'country/cl_weekly.html',\n 'country/co_weekly.html',\n 'country/cr_weekly.html',\n 'country/cy_weekly.html',\n 'country/cz_weekly.html',\n 'country/dk_weekly.html',\n 'country/do_weekly.html',\n 'country/ec_weekly.html',\n 'country/eg_weekly.html',\n 'country/sv_weekly.html',\n 'country/ee_weekly.html',\n 'country/fi_weekly.html',\n 'country/fr_weekly.html',\n 'country/de_weekly.html',\n 'country/gr_weekly.html',\n 'country/gt_weekly.html',\n 'country/hn_weekly.html',\n 'country/hk_weekly.html',\n 'country/hu_weekly.html',\n 'country/is_weekly.html',\n 'country/in_weekly.html',\n 'country/id_weekly.html',\n 'country/ie_weekly.html',\n 'country/il_weekly.html',\n 'country/it_weekly.html',\n 'country/jp_weekly.html',\n 'country/kz_weekly.html',\n 'country/lv_weekly.html',\n 'country/lt_weekly.html',\n 'country/lu_weekly.html',\n 'country/my_weekly.html',\n 'country/mt_weekly.html',\n 'country/mx_weekly.html',\n 'country/ma_weekly.html',\n 'country/nl_weekly.html',\n 'country/nz_weekly.html',\n 'country/ni_weekly.html',\n 'country/ng_weekly.html',\n 'country/no_weekly.html',\n 'country/pk_weekly.html',\n 'country/pa_weekly.html',\n 'country/py_weekly.html',\n 'country/pe_weekly.html',\n 'country/ph_weekly.html',\n 'country/pl_weekly.html',\n 'country/pt_weekly.html',\n 'country/ro_weekly.html',\n 'country/ru_weekly.html',\n 'country/sa_weekly.html',\n 'country/sg_weekly.html',\n 'country/sk_weekly.html',\n 'country/za_weekly.html',\n 'country/kr_weekly.html',\n 'country/es_weekly.html',\n 'country/se_weekly.html',\n 'country/ch_weekly.html',\n 'country/tw_weekly.html',\n 'country/th_weekly.html',\n 'country/tr_weekly.html',\n 'country/ua_weekly.html',\n 'country/ae_weekly.html',\n 'country/uy_weekly.html',\n 'country/ve_weekly.html',\n 'country/vn_weekly.html']\n\n\nWe cleaned the data by iterating over a link_list obtained from scraping Spotify weekly charts from the website ‘https://kworb.net/spotify/’. We extracted and processed Spotify weekly chart data from the specified URLs, splitting artist and song title information and dropping unnecessary columns and displayed the resulting data frame.\nfor link in link_list[1:3]:\n    df = pd.read_html(f'https://kworb.net/spotify/{link}')[0]\n\n    if 'Artist and Title' in df.columns: \n        df[['artist_name', 'song_title']] = df['Artist and Title'].str.split('-', n=1, expand = True) \n        df.drop(columns=['Artist and Title'], inplace=True)\n        df.drop(columns=['P+', 'Wks', 'Pk', '(x?)', 'Streams', 'Streams+', 'Total'], inplace = True) \n\n    display(df) \n\n\n\noutput\n\n\nNext, we extracted country names from specific tags in the HTML content parsed using BeautifulSoup. It filters out certain country names specified in the filters list and then prints the remaining country names.\ntd_tags = soup.find_all('td', class_=\"mp text\")\nfilters = ['Global'] \ncountry_names_list = []\n\nfor i in range(0, len(td_tags), 2):\n    country_name = td_tags[i].get_text()\n    \n    if country_name not in filters:\n        country_names_list.append(country_name) \n\nprint(country_names_list)\n['United States', 'United Kingdom', 'Andorra', 'Argentina', 'Australia', 'Austria', 'Belarus', 'Belgium', 'Bolivia', 'Brazil', 'Bulgaria', 'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Cyprus', 'Czech Republic', 'Denmark', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Guatemala', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Malta', 'Mexico', 'Morocco', 'Netherlands', 'New Zealand', 'Nicaragua', 'Nigeria', 'Norway', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Russia', 'Saudi Arabia', 'Singapore', 'Slovakia', 'South Africa', 'South Korea', 'Spain', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Turkey', 'Ukraine', 'United Arab Emirates', 'Uruguay', 'Venezuela', 'Vietnam']\nFinally, we created a combined dataframe with the columns Pos, artist_name, song_title, and country. We saved the dataframe to a csv file.\ncombined_df = pd.DataFrame()\n\nfor link, country_name in zip(link_list[1:], country_names_list):\n    df = pd.read_html(f'https://kworb.net/spotify/{link}')[0]\n\n    if 'Artist and Title' in df.columns: \n        df[['artist_name', 'song_title']] = df['Artist and Title'].str.split('-', n=1, expand = True) \n        df.drop(columns=['Artist and Title', 'P+', 'Wks', 'Pk', '(x?)', 'Streams', 'Streams+', 'Total'], inplace = True) \n        \n    df['country'] = country_name\n    \n    combined_df = combined_df.append(df, ignore_index = True) \n\ncombined_df.to_csv('country_charts.csv', index=False)"
  },
  {
    "objectID": "posts/Final Project Submission/index.html#sql-database-countries_sql.py",
    "href": "posts/Final Project Submission/index.html#sql-database-countries_sql.py",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "First, we imported the necessary libraries to create our database.\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nNext, we created a SQL Database that is organized by country name. To do this, we read the country_charts.csv file through pandas, then convert the dataframe to a SQLite database. We also defined a class DatabaseHandler with static methods for creating a SQLite database, creating a table within that database, and inserting data into the table.\ndf = pd.read_csv('/Users/andrewhan/Desktop/2023-2024/Winter_24/PIC_Class/Project/country_charts.csv')\ndf.head()\n\nconn = sqlite3.connect('country_data.db')\n\ndf.to_sql('country_data', conn, if_exists='replace', index=False)\nThe code below defines a class DatabaseHandler with static methods for creating a SQLite database, creating a table within that database, and inserting data into the table. It iterates through the CSV that we read as a dataframe and inserts the data into the db using the query seen below.\nclass DatabaseHandler:\n    @staticmethod\n    def create_database():\n        conn = sqlite3.connect('country_data.db') #creates and connects to country_data.db database\n        conn.close()\n    \n\n    @staticmethod\n    def create_table():\n        conn = sqlite3.connect('country_data.db')\n        c = conn.cursor()\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS countries (\n            country TEXT,\n            song_name TEXT,\n            artist_name TEXT,\n            rank INTEGER\n                )\n        ''')\n        conn.commit()\n        conn.close()\n\n# MULTI INDEX SQL\n    @staticmethod\n    def insert_data():\n        DatabaseHandler.create_database()\n        DatabaseHandler.create_table()\n        conn = sqlite3.connect('country_data.db')\n        c = conn.cursor()\n        c.execute('''\n            INSERT INTO countries (country, song_name, artist_name, rank)\n            SELECT country, song_name, artist_name, rank\n            FROM countries\n            WHERE (country, rank) IN (\n            SELECT country, MAX(rank)\n            FROM countries\n            GROUP BY country\n            )\n        ''')\n        conn.commit()\n        \n        #fetch data from the countries table\n        c.execute('''\n            SELECT c.country, c.song_name, c.artist_name, c.rank\n            FROM countries c\n            JOIN countries yt ON c.country = yt.country AND c.song_name = yt.song_name AND c.artist_name = yt.artist_name\n            ORDER BY c.country\n        ''')\n\n        conn.commit()\n        conn.close()\n\nDatabaseHandler.insert_data()\n\n\nTo authorize Spotify, we imported the spotipy library.\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\nNext, we defined the functions the collect user listening data and recommend countries based on the user’s top songs.\ndef collect_user_listening_data():\n    # Initialize Spotipy with your client ID, client secret, and redirect URI\n    sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id='your_client_id',\n                                                   client_secret='your_client_secret',\n                                                   redirect_uri='your_redirect_uri',\n                                                   scope='user-library-read,user-top-read'))\n\n    # Fetch the user's top artists and tracks\n    top_artists = sp.current_user_top_artists(limit=15)\n    top_tracks = sp.current_user_top_tracks(limit=200)\n\n    # Process the fetched data as needed\n    user_listening_data = {\n        'top_artists': [artist['name'] for artist in top_artists['items']],\n        'top_tracks': [track['name'] for track in top_tracks['items']]\n    }\n    return user_listening_data\n\ndef recommend_countries(user_top_songs):\n    conn = sqlite3.connect('country_data.db')\n    c = conn.cursor()\n\n    country_counts = {}\n\n    for song in user_top_songs:\n        # Query the database to retrieve associated countries\n        c.execute(\"SELECT DISTINCT country FROM countries WHERE song_name = ?\", (song,))\n        countries = c.fetchall()\n        \n        # Increment counts for each country\n        for country in countries:\n            country_name = country[0]\n            country_counts[country_name] = country_counts.get(country_name, 0) + 1\n    \n    # Close connection\n    conn.close()\n    \n    # Recommend the country with the highest count\n    top_countries = sorted(country_counts, key=country_counts.get, reverse=True)[:3]\n\n    total_top_country_occurrences = sum(country_counts[country] for country in top_countries)\n    \n    # Calculate the country score as a percentage of how much the user's top songs match the top countries\n    country_scores = {country: (country_counts[country] / total_top_country_occurrences) * 100 for country in top_countries}\n    \n    return top_countries, country_scores"
  },
  {
    "objectID": "posts/Final Project Submission/index.html#country-recommendation-function",
    "href": "posts/Final Project Submission/index.html#country-recommendation-function",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "The recommend_countries function works by comparing the user’s top songs with the top songs of each individual country. By utilizing the SQLite query, we were able to make the algorithm relatively efficient (compared to iterating through every single user’s songs and each country’s songs). It finds the “DISTINCT” songs from each country, and creates a score based off the total number of distinct song occurrences in the user’s top songs."
  },
  {
    "objectID": "posts/Final Project Submission/index.html#visualizing-the-data-visualization.py",
    "href": "posts/Final Project Submission/index.html#visualizing-the-data-visualization.py",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "First, we import the necessary libraries for visualizing the data.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport csv\n\n# data visualization \nimport plotly.express as px \n\n# Import your functions or ensure they are defined in this script\n# from your_module import collect_user_listening_data, recommend_countries\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\n\nimport dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nWe created three functions to visualize the data. plot_top_songs takes in the user’s top songs and generates a horizontal bar plot using Plotly Express. Similar to the previous function, plot_top_artists takes a list of user’s top artists as input and generates a horizontal bar plot using Plotly Express. plot_recommended_countries takes a dictionary of country scores as input and generates a scatter plot using Plotly Express.\n# ===== Visualization 1: User's Top Tracks ===== (works)\ndef plot_top_songs(user_top_songs): \n    \"\"\"\n    This plot shows the top 50 songs for the user\n    \"\"\"\n\n    # putting the list of songs into another variable  \n    songs = user_top_songs\n\n    # creating a dataframe \n    top_songs_data = pd.DataFrame()\n\n    # processing and concatenating to clean up the dataframe \n    top_songs_data['Songs'] = songs\n\n    # Adding position values to prepare for the grapg \n    top_songs_data['Position'] = top_songs_data.index + 1\n\n    # creating the plot \n    top_songs_data = top_songs_data[::-1]\n    fig = px.bar(top_songs_data, x = 'Position', y = 'Songs', orientation='h', width=1500, height=1000)\n    fig.update_layout(\n        margin=dict(l=50, r=50, t=50, b=50),  # Adjust margins as needed\n    )\n\n    return fig\n\n# ===== Visualization 2: User's Top Artists ====== (works)\ndef plot_top_artists(user_top_artists):\n    \"\"\"\n    This plot shows the top 15 artists for the user\n    \"\"\"\n\n    # putting the list of songs into another variable  \n    artists = user_top_artists\n\n    # creating a dataframe \n    top_artists_data = pd.DataFrame()\n\n    # processing and concatenating to clean up the dataframe \n    top_artists_data['Artists'] = artists\n\n    # Adding position values to prepare for the grapg \n    top_artists_data['Position'] = top_artists_data.index + 1\n\n    # creating the plot \n    # top_artists_data = top_artists_data[::-1]\n    fig = px.bar(top_artists_data, x = 'Position', y = 'Artists', orientation='h', width=1500, height=700, color='Artists')\n    fig.update_layout(\n        margin=dict(l=15, r=15, t=15, b=15),\n\n    )\n    \n    return fig\n\n# ===== Visualization 3: Recommended Countries Plot ====== (works)\ndef plot_recommended_countries(country_scores): \n    # first processing into a dataframe for the plot \n    # creating new columns, preparing for df \n    column_names = ['Country', 'Common_Songs']\n    \n    # creating a new dataframe \n    recommend_data = pd.DataFrame(list(country_scores.items()), columns=column_names)\n\n\n    #feeding into new dataframe\n    # assuming that country_scores is a list of country names \n    # countries = country_scores \n    fig = px.scatter(recommend_data, x=\"Country\", y=\"Common_Songs\")\n    fig.show()\n\n    return fig"
  },
  {
    "objectID": "posts/Final Project Submission/index.html#web-development-plotlydash.py",
    "href": "posts/Final Project Submission/index.html#web-development-plotlydash.py",
    "title": "PIC16B Project Group 3",
    "section": "",
    "text": "We created a Plotly Dash web application that allows users to visualize different aspects of their music preferences. The app consists of a dropdown menu to select the type of data to visualize (top songs, top artists, or recommended countries) and a graph area where the selected data will be displayed.\n\n\n\noutput\n\n\nimport plotly.express as px \n\n# Import your functions or ensure they are defined in this script\n# from your_module import collect_user_listening_data, recommend_countries\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\n\nimport dash\nfrom dash import dcc, html\nimport dash_ag_grid as dag\nfrom dash.dependencies import Input, Output\n\n# from visualization \nfrom visualization import plot_top_songs, plot_top_artists, plot_recommended_countries\n\n# ==== PLOTLY APP ====\n\n# Define the Plotly Dash app\napp = dash.Dash(__name__)\n\n# Define the layout of the app\napp.layout = html.Div(\n    style={'display': 'flex', 'justifyContent': 'center', 'alignItems': 'center', 'flexDirection': 'column', 'backgroundColor': '#1db954'},\n    children=[\n    html.H1(children='User Vs. The World', style={'textAlign':'center', 'fontSize':50, 'fontFamily': 'Gill Sans', 'color': 'White', 'marginBottom': '10px', 'fontWeight': 'medium'}),\n    html.P(\"What country do your music tastes align with?\", style={'fontFamily': 'Gill Sans', 'fontSize': 20, 'color':'191414', 'marginTop':'10px'}),\n    dcc.Dropdown(\n        id='data-type-dropdown',\n        options=[\n            {'label': 'Top Songs', 'value': 'top_songs'},\n            {'label': 'Top Artists', 'value': 'top_artists'},\n            {'label': 'Similar Countries', 'value': 'recommended_countries'},\n        ],\n        value='top_songs',  # Default value\n        clearable=False,\n        style={'width': '300px','margin': 'auto', 'fontSize': '20px', 'fontFamily': 'Gill Sans'},\n    ),\n    dcc.Graph(id='data-plot', style={'margin': 'center', 'marginTop': '20px'})\n    ]\n)\n\n# Define a callback to update the plot based on the dropdown selection\n@app.callback(\n    Output('data-plot', 'figure'),\n    [Input('data-type-dropdown', 'value')]\n)\n\ndef update_data_plot(data_type):\n    if data_type == 'top_songs':\n        # Call function to collect user's top songs\n        user_top_songs = collect_user_top_tracks()  # Assuming you have a function like this\n        \n        # Call function to plot top songs\n        return plot_top_songs(user_top_songs)\n    \n    elif data_type == 'top_artists':\n        # Call function to collect user's top artists\n        user_top_artists = collect_user_top_artists()  # Assuming you have a function like this\n        \n        # Call function to plot top artists\n        return plot_top_artists(user_top_artists)\n    \n    elif data_type == 'recommended_countries':\n        user_top_songs = collect_user_top_tracks()\n        recommended = recommend_countries(user_top_songs)\n\n        return plot_recommended_countries(recommended)\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\nTo summarize, our project correlates a Spotify user’s listening data with a specific country. We collected data from global charts through web scraping, utilized a SQL database for data management, and showcased the insights through visualizations on a Plotly Dash website. While our project is generally free from ethical or environmental concerns, it’s essential to acknowledge potential privacy implications related to sharing user data through Spotify authorization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC16B",
    "section": "",
    "text": "PIC16B Project Group 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification Tutorial\n\n\n\n\n\n\nweek 8\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nMessage bank Tutorial\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2 - Scrapy\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Tutorial\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nAndrew Han\n\n\n\n\n\n\nNo matching items"
  }
]