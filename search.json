[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index 2.html",
    "href": "index 2.html",
    "title": "Homework 2 - Scrapy",
    "section": "",
    "text": "Introduction\nhttps://andrewshan214.github.io/PIC16B/posts/Homework%20#2%20-%20Scrapy/index.ipynb\nIn this blog post, I will be giving a tutorial on how to write a spider in scrapy to scrape information from a movie database. More specifically, I will be demonstrating using Christopher Nolan’s “The Dark Knight”, one of my favorite movies. Using the data that I scrape, I will create a chart to suggest movies based on the actors in “The Dark Knight”.\n\n\nWriting the movie database parse() function\nFirst we write the initial parse() function, shown below:\n\ndef parse(self, response):\n        cast_crew_url = f\"https://www.themoviedb.org/movie/155-the-dark-knight/cast\"\n        yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\nThis method works by saving the url of the full cast and crew page of the movie website. I use scrapy’s built in functions to go into this website and call a second parse function that will save all of the nanmes of those that acted in the movie. The implementation of the second parse function is below.\n\n\nparse_full_credits function implementation\n\ndef parse_full_credits(self, response):\n        for cast_member in response.css(\"ol.people.credits li\"):\n            actor_url = cast_member.css('a::attr(href)').get()\n\n            if actor_url:\n                yield scrapy.Request(url=response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFirstly, I iterated through all of the actors by using the css identifier of only actors by looking at the html code in the movie cast website. While iterating, I saved each actor’s unique page url in a variable. Using that url variable, I used Scrapy’s request function to go into the actor’s page.\nIn this page, I called a third parse function.\n\n\nparse_actor_page function implementation\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('div.title h2.title a::text').get()\n\n        acting_roles = response.css('div.credits_list h3.zero:contains(\"Acting\")')\n        \n        if acting_roles:\n            for acting_role in acting_roles.xpath('./following-sibling::table[@class=\"card credits\"]//table[@class=\"credit_group\"]'):\n                movie_name = acting_role.css('td.role a.tooltip bdi::text').get()\n\n                if movie_name:\n                    yield {\n                        'actor': actor_name,\n                        'movie_or_TV_name': movie_name,\n                    }\n\nI implemented this by first getting the actor’s name by using the css identifier from the website.\nNext, I sorted through only this actor’s acting roles by using the css identifier for acting roles, which was “Zero”. I iterated through all of these acting roles, and got each movie name, and yielded them into a dictionary of the actor’s name and the movies they’ve acted in."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello classmates! I’m a third year Mathematics/Economics Major, with a specialization in Computing.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Homework 5 - Image Classification/index.html",
    "href": "posts/Homework 5 - Image Classification/index.html",
    "title": "Image Classification Tutorial",
    "section": "",
    "text": "Introduction\nThe url for this blog is: https://andrewshan214.github.io/PIC16B/\nThe url for the github repo is: https://github.com/andrewshan214/PIC16B\nIn this blog post, I will be giving a short tutorial on how to implement an image classification model to distinguish between photos of dogs and cats.\nFirst, we import the proper packages in Python to develop our model. (See below)\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n\n\nCreating the dataset\nBelow is code that is used to create datasets for training, validation, and testing.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\nResizing the images\nBelow, we write code to resize all of the images to a standard size of (150, 150), as well as to rapidly read data by altering the batch_size to 64.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\nVisualizing the dataset\nBelow, I have implemented a function called visualize_dataset) that creates two lists, and adds 3 random images of dogs and cats in each, respectively. i use the “take” method to get images of each animal from the dataset to fill the lists.\nThen I iterate through the lists to create a plot that displays 3 random cats in the first row and 3 random dogs in the second row, using the pyplot library.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_dataset(dataset, num_samples=3, title=\"\"):\n    \n    plt.figure(figsize=(15, 6))\n\n    # initialize empty sets for each animal\n    cat_images, dog_images = [], []\n    #for loop to iterate through each image in the dataset\n    for images, labels in dataset.take(1):\n        for image, label in zip(images, labels):\n            if label == 0:\n                cat_images.append(image.numpy())\n            else:\n                dog_images.append(image.numpy())\n\n    for i in range(num_samples):\n        # Plot cat images in the first row\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(cat_images[i].astype(\"uint8\"))\n        plt.title(\"Cat\")\n        plt.axis(\"off\")\n\n        # Plot dog images in the second row\n        plt.subplot(2, num_samples, i + num_samples + 1)\n        plt.imshow(dog_images[i].astype(\"uint8\"))\n        plt.title(\"Dog\")\n        plt.axis(\"off\")\n\n    plt.show()\n\n\n\nvisualize_dataset(train_ds, title=\"Random Samples from Training Dataset\")\n\n2024-03-01 21:30:02.241252: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\n\n\n\n\n\n\n\n\nChecking Label Frequencies\nBelow, we’ve created an iterator that can go through the dataset, which we’ve used to count the total number of images of both cats and dogs by using a simple for loop.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncat_count = 0\ndog_count = 0\n\n# Iterate through labels\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    elif label == 1:\n        dog_count += 1\n\nprint(\"Number of images with label 0 (cat):\", cat_count)\nprint(\"Number of images with label 1 (dog):\", dog_count)\n\nNumber of images with label 0 (cat): 4637\nNumber of images with label 1 (dog): 4668\n\n\n\n\nCreating model1\nBelow we implemented our first model, which includes different layers to the model. Each layer processes a bit of the input data and produces an output. Each layer consists of a set of neurons that perform specific computation on the input data.\nAfter implementing the layers into model1, we compile and fit the model to evaluate the test and validation accuracy.\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\n# history = model1.fit(train_ds.batch(batch_size), \n#                       epochs=20, \n#                       validation_data=validation_ds.batch(batch_size))\n\nEpoch 1/20\n146/146 [==============================] - 286s 2s/step - loss: 9.3475 - accuracy: 0.5050 - val_loss: 0.6930 - val_accuracy: 0.5009\nEpoch 2/20\n 85/146 [================&gt;.............] - ETA: 1:57 - loss: 0.6930 - accuracy: 0.4996\n\n\nKeyboardInterrupt: \n\n\nSome things I experimented with was increasing the model complexity by adding more layers. Additionally, I tried to regularized the data to prevent overfitting.\nThe validation accuracy of my model stabilized between 60% and 61% during training, which is higher than the baseline.\n\n\nModel with Data Augmentation\nBy augmenting the data, it allows the model to recognize images, even if they’re rotated or mirrored.\nThe random flip layer is implemented, by randomly mirroring images, and initializing the layer to that.\nSimilarly, the random rotation layer by randomly rotating the images, and training a layer to the random rotations.\nI’ve added before and after photos for the effect of both layers for visual aid. See code and images below.\n\n#random_flip_layer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Load a few sample images\nsample_images = []\nfor image, label in train_ds.take(1):\n    sample_images = image.numpy()[:4]  # Take the first three images\n\n# Normalize pixel values to the range [0, 1]\nsample_images = sample_images.astype(\"float32\") / 255.0\n\n# Create a RandomFlip layer\nrandom_flip_layer = tf.keras.layers.RandomFlip(\"horizontal\")\n\n# Apply RandomFlip to the sample images\naugmented_images = random_flip_layer(sample_images)\n\n# Plot original and augmented images\nplt.figure(figsize=(10, 7))\nfor i in range(0, 3):\n    # Original image\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(sample_images[i])\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    # Augmented image\n    plt.subplot(2, 3, i + 4)\n    plt.imshow(augmented_images[i])\n    plt.title(\"Augmented\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n#RandomRotation layer\n\nsample_images = []\nfor image, label in train_ds.take(1):\n    sample_images = image.numpy()[:3]  # Take the first three images\n\n# Normalize pixel values to the range [0, 1]\nsample_images = sample_images.astype(\"float32\") / 255.0\n\n# Create a RandomRotation layer\nrandom_rotation_layer = tf.keras.layers.RandomRotation(factor=0.5)\n\n# Apply RandomRotation to the sample images\naugmented_images = random_rotation_layer(sample_images)\n\n# Plot original and augmented images\nplt.figure(figsize=(10, 7))\nfor i in range(0, 3):\n    # Original image\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(sample_images[i])\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    # Augmented image\n    plt.subplot(2, 3, i + 4)\n    plt.imshow(augmented_images[i])\n    plt.title(\"Augmented\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel 2\nAfter creating those two layers above, we include them in the second model (in addition to all the other layers from model1) to hopefully find a more accurate model. See code below.\n\n#creating model2 with augmentation layers\n\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(factor=0.2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile model2\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train model2\nhistory2 = model2.fit(train_ds, \n                      epochs=20, \n                      validation_data=validation_ds)\n\nThe accuracy of my model stabilized between 67% and 68% during training, which is a bit higher than in model1, and still higher than 60%.\nIn model2, if the accuracy stabilized around 67% to 68% during training and the validation accuracy closely tracked the training accuracy without diverging significantly, it indicates that the model may not be overfitting. Signs of overfitting include a large gap between training and validation accuracy or when the validation accuracy starts decreasing while the training accuracy continues to increase.\n\n\nModel 3\nWe implemented model 3 hoping to make training the model faster by implemented a preprocessor layer that would handle the scaling of the RGB values prior to the training process. This allows the computer to focus more energy on handling actual signal in the data and less energy having the weights adjust to the data scale.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = [i], outputs = [x])\n\nmodel3 = tf.keras.Sequential([\n    preprocessor,\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(factor=0.2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile model3\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\nhistory3 = model3.fit(train_ds, \n                      epochs=20,  # Increase the number of epochs\n                      validation_data=validation_ds)\n\nThe validation accuracy stabilized between 80% and 82% during training This is much higher than the val_accuracy of model1, which was ~60%.\nSince there is not a significant gap between the training and validation accuracy, it suggests that model3 is not overfitting the data.\n\n\nTransfer Learning\nTransfer learning is essentially accessing a pre-existing base model and incorporating it into our own model. The first few lines of the code below is downloading MobileNetV3Large, which will serve as our base model. We create a base_model_layer that we implement into our model4, which is constructed in the same manner as the previous 3 models.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights=None)\nweights_path = \"/Users/andrewhan/Desktop/2023-2024/Winter_24/PIC_Class/Homework/HW5/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\"\nbase_model.load_weights(weights_path)\n\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(factor=0.2)\n])\n\nmodel4 = tf.keras.Sequential([\n    data_augmentation,\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(2, activation='softmax')  \n])\n\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\nhistory4 = model4.fit(train_ds, \n                      epochs=20,  # Increase the number of epochs\n                      validation_data=validation_ds)\n\nThe model4 validation accuracy stabilzed between 97% and 98%, which is greater than the required 93% This val_accuracy is also much higher than from model1, which was around 60%.\nThere does not seem to be any overfitting as there is not much difference between the test and validation accuracy.\n\n\nScore on Test Data\nThis code below will evaluate the accuracy of model4, which is the most performant model of the four. This will evaluate the unseen test dataset and print the value it returns.\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\n\n# Print the test accuracy\nprint(f'Test accuracy: {test_accuracy}')\n\n37/37 [==============================] - 8s 198ms/step - loss: 0.0733 - accuracy: 0.9729\nTest accuracy: 0.9729148745536804\n\n\nThe test accuracy was over 97%, so we can see that the model works well. This suggests that the model has effectively learned to distinguish between cats and dogs in the test dataset."
  },
  {
    "objectID": "posts/Palmer Penguins/index.html",
    "href": "posts/Palmer Penguins/index.html",
    "title": "Palmer Penguins Tutorial",
    "section": "",
    "text": "Blog Post URL https://andrewshan214.github.io/PIC16B/posts/Palmer%20Penguins/\nIn this blog post, I will be giving a short tutorial on how to make a simple data visualization of the “Palmer Penguins” data set. More specifically, I will be showing a simple and informative visualization as a scatter plot to visualize the relationship between penguin culmen length and culmen depth.\nFirst, we load the necessary packages and data, as shown below.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = \"/Users/andrewhan/PIC16B/posts/Palmer Penguins/palmer_penguins.csv\"\npenguins = pd.read_csv(data)\n\nNext, we will filter our data set to drop any entries with NaN values for their culmen length and/or depth.\nUsing that filtered data, we will group the data based on those categories. Then we will plot the data using the matplotlib package. Using key words, we can choose the type of chart we want, as well as the colors. In this case, I went with a scatter plot with blue dots.\nAfter adding the titles and labels, we can display our data visualization!\n\npenguins_filtered = penguins.dropna(subset = ['Culmen Length (mm)', 'Culmen Depth (mm)'])\n\nplt.figure(figsize=(10, 6))\nplt.scatter(penguins['Culmen Length (mm)'], penguins['Culmen Depth (mm)'], c='blue', alpha=0.7)\nplt.title('Scatter Plot of Culmen Length vs Culmen Depth')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, there is scatter plot with all of our data points (penguins) plotted with their respective culmen length and depth. It seems that there is no identifiable trend between their length and depth measurements as there are many penguins with both proportionally larger culmen lengths and culmen depths."
  },
  {
    "objectID": "posts/Homework3-Message_Bank/index.html",
    "href": "posts/Homework3-Message_Bank/index.html",
    "title": "Message bank Tutorial",
    "section": "",
    "text": "Introduction\nThe url for this blog is: https://andrewshan214.github.io/PIC16B/\nThe url for the github repo is: https://github.com/andrewshan214/PIC16B\nIn this blog post, I will be giving a short tutorial on how to make a simple message bank using Python’s Flask library, as well as incorporating HTML and CSS files to develop a webpage\nFirst, we create the necessary files. To start we will create our app.py file, which will be a Python script file that will be running the Flask library tasks. Please see below.\n\nfrom flask import Flask, render_template, request\nfrom flask import redirect, url_for, abort\nfrom flask import g\nimport sqlite3\n\napp = Flask(__name__)\n\n\n\nGoing through app.py file\nAlong with importing the necessary libraries, we must define a few functions and databases for our message bank. Please see the comments above each function to understand what each function is doing in the context of the webpage.\n\n# Create a sqlite3 database to handle all of the submitted messages\nDATABASE = 'messages_db.sqlite'\n\n# This function checks for the existence of a message_db in the global 'g' object. It then closes the database connection if it sees its existence.\n@app.teardown_appcontext\ndef close_db(error):\n    if hasattr(g, 'message_db'):\n        g.message_db.close()\n\n@app.route('/') # @app.route essentially tells the code what the end of the url of this particular page would be. \n# Because this is the base 'Home' page, we use a simple '/' to conclude the url.\ndef base():\n    #Uses Flask's render_template function to render the base.html file\n    return render_template('base.html')\n\n\n\nConnection between app.py and each page’s html file.\nAfter defining a base() function, we must accompany it with an html file to tell the website what to display on this page. See below for the code for base.html.\n\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} - PIC16B Website&lt;/title&gt;\n&lt;nav&gt;\n    &lt;h1&gt;PIC16B Message Bank!&lt;/h1&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('base') }}\"&gt;Main page&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('submit_message')}}\"&gt;Submit Message&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('render_view_template') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;nav&gt;\n&lt;section class=\"content\"&gt;\n    &lt;header&gt;\n    {% block header %}{% endblock %}\n    &lt;/header&gt;\n    {% block content %}{% endblock %}\n&lt;/section&gt;\n\n\n\nbase.html\nWhat this code is doing is both serving as a base for additional html files, as well as formatting the base page. It lays out a page title, as well as navigation functionality to visit other pages. See below for the app.py function that renders the message submission page, as well as the accompanying submit.html file.\n\n# Like before, we use @app.route. \n#However, now we must define the methods to ensure that user submission is possible by adding methods=['GET, POST']\n\n@app.route('/submit/', methods=['GET', 'POST'])\ndef submit_message():\n    #different methods for diff request methods\n    if request.method == 'GET':\n        #if 'GET' just render the template\n        return render_template('submit.html')\n    \n    else:\n        try:\n            handle, message = insert_message(request)\n            #render the template with a thank you note\n            return render_template('submit.html', thank_you = True, message = message, handle = handle)\n        except:\n            return render_template('submit.html', error=True)\n\n\n{% extends \"base.html\" %} \n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n    &lt;form action=\"/submit\" method=\"post\"&gt;\n        &lt;label for=\"message\"&gt;Message:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"message\" name=\"message\"&gt;&lt;br&gt;\n        &lt;label for=\"name\"&gt;Name:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"name\" name=\"name\"&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit\"&gt;\n    &lt;/form&gt;\n{% endblock %}\n\n\n\nMessage Submission Page\nThis page is run by the function submit_message(), which essentially receives a message from the user and renders the submit.html page. The submit.html file “extends” the base.html file, which essentially means that many components, like the style.css, is automatically transferred from the base.html file to submit.html. See below for a screenshot of this page.\n\n\n\nMessageSubmission\n\n\n\n\nDatabase Management\nSee below for the code for inserting a message into the SQL message database.\n\n@app.route('/getmsg/', methods=['GET'])\ndef get_message_db():\n\n    #try to retrieve database connection from global g object\n    try:\n        db = g.message_db\n    except AttributeError:\n        db = g.message_db = sqlite3.connect(DATABASE)\n        db.execute('''\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY,\n                handle TEXT,\n                message TEXT\n            )\n        ''')\n    return db\n\n@app.route('/insert_msg/', methods=['POST'])\ndef insert_message(request):\n    # get message and handle from request\n    try:\n        message = request.form['message']\n        handle = request.form['name']\n\n        db = get_message_db()\n        cursor = db.cursor()\n        cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n        db.commit()\n\n        return redirect('/submit/')\n    except Exception as e:\n        return render_template('submit.html', error=True)\n\n\n\ninsert_message(request)\nWhat this function does is connect to the message database and inserts the user-inputted message into the database so that it can be displayed in the message viewing page.\nSee below for how the message viewing page is run.\n\n\nRandom Messages Page\n\n@app.route('/random_messages/&lt;int:n&gt;')\ndef random_message(n):\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    return render_template('view.html', messages=messages)\n\n\n@app.route('/random_messages/')\ndef render_view_template():\n    messages = random_message(5)\n\n    return render_template('view.html', messages = messages)\n\nThese functions connect to the message database to return and render a list of previously inputted, randomly selected messages to the user. See below for the view.html file.\n\n{% extends \"base.html\" %}\n\n{% block content %}\n    &lt;h2&gt;Random Messages&lt;/h2&gt;\n    &lt;ul&gt;\n        {% for message in messages %}\n            &lt;li&gt;\n                &lt;strong&gt;{{ message.handle }}&lt;/strong&gt; - {{ message.message }}\n            &lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n{% endblock %}\n\n\n\nview.html\nThis code also “extends” base.html. This html code is much simpler as it simply lists the previous messages, and doesn’t need to receive any information from the website user. See an example below.\n\n\n\nRandomMessages\n\n\n\n\nstyle.css\nWe have now seen how the website functions on the back-end with the database management and user inputs. However, this must all be presentable. As I mentioned earlier, the base.html file is accompanies by a style.css file. See below for the css code.\n\n/* Import a custom font from Google Fonts */\n@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');\n\n/* Apply the custom font to the entire body */\nbody {\n    font-family: 'Roboto', sans-serif;\n    background-color: #f0f0f0; /* Light gray background */\n    color: #333; /* Dark gray text */\n}\n\n/* Style the navigation bar */\nnav {\n    background-color: #333; /* Dark background color */\n    color: white; /* White text */\n    padding: 10px;\n}\n\n/* Style navigation links */\nnav a {\n    color: white; /* White text */\n    text-decoration: none; /* Remove underline */\n    margin-right: 10px; /* Add some margin between links */\n}\n\n/* Style the header section */\nheader {\n    background-color: #007bff; /* Blue background color */\n    color: white; /* White text */\n    padding: 20px;\n    text-align: center; /* Center align text */\n}\n\n/* Add some margin around the content */\n.content {\n    margin: 20px;\n}\n\n/* Style forms */\nform {\n    margin-bottom: 20px; /* Add margin below forms */\n}\n\n/* Style text inputs and buttons */\ninput[type=\"text\"], button {\n    padding: 10px;\n    margin-right: 10px;\n}\n\n/* Style buttons */\nbutton {\n    background-color: #007bff; /* Blue background color */\n    color: white; /* White text */\n    border: none; /* Remove border */\n    cursor: pointer; /* Change cursor to pointer */\n}\n\n/* Change button color on hover */\nbutton:hover {\n    background-color: #0056b3; /* Darker blue on hover */\n}\n\n/* Style unordered lists */\nul {\n    list-style-type: none; /* Remove bullet points */\n    padding: 0; /* Remove default padding */\n}\n\n/* Add some margin below list items */\nli {\n    margin-bottom: 10px;\n}\n\nThis code goes through many different customizations, such as font type, font size and color, page color, etc.\n\n\nConclusion\nAs we can see, there are many different things to account for in web development. Hopefully, this blog post was informational and helpful in developing your next webpage. Thank you!\n\nAndrew Han"
  },
  {
    "objectID": "posts/Homework #2 - Scrapy/index.html",
    "href": "posts/Homework #2 - Scrapy/index.html",
    "title": "Homework 2 - Scrapy",
    "section": "",
    "text": "Introduction\nhttps://andrewshan214.github.io/PIC16B/posts/Homework%20#2%20-%20Scrapy/index.ipynb\nIn this blog post, I will be giving a tutorial on how to write a spider in scrapy to scrape information from a movie database. More specifically, I will be demonstrating using Christopher Nolan’s “The Dark Knight”, one of my favorite movies. Using the data that I scrape, I will create a chart to suggest movies based on the actors in “The Dark Knight”.\n\n\nWriting the movie database parse() function\nFirst we write the initial parse() function, shown below:\n\ndef parse(self, response):\n        cast_crew_url = f\"https://www.themoviedb.org/movie/155-the-dark-knight/cast\"\n        yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\nThis method works by saving the url of the full cast and crew page of the movie website. I use scrapy’s built in functions to go into this website and call a second parse function that will save all of the nanmes of those that acted in the movie. The implementation of the second parse function is below.\n\n\nparse_full_credits function implementation\n\ndef parse_full_credits(self, response):\n        for cast_member in response.css(\"ol.people.credits li\"):\n            actor_url = cast_member.css('a::attr(href)').get()\n\n            if actor_url:\n                yield scrapy.Request(url=response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFirstly, I iterated through all of the actors by using the css identifier of only actors by looking at the html code in the movie cast website. While iterating, I saved each actor’s unique page url in a variable. Using that url variable, I used Scrapy’s request function to go into the actor’s page.\nIn this page, I called a third parse function.\n\n\nparse_actor_page function implementation\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('div.title h2.title a::text').get()\n\n        acting_roles = response.css('div.credits_list h3.zero:contains(\"Acting\")')\n        \n        if acting_roles:\n            for acting_role in acting_roles.xpath('./following-sibling::table[@class=\"card credits\"]//table[@class=\"credit_group\"]'):\n                movie_name = acting_role.css('td.role a.tooltip bdi::text').get()\n\n                if movie_name:\n                    yield {\n                        'actor': actor_name,\n                        'movie_or_TV_name': movie_name,\n                    }\n\nI implemented this by first getting the actor’s name by using the css identifier from the website.\nNext, I sorted through only this actor’s acting roles by using the css identifier for acting roles, which was “Zero”. I iterated through all of these acting roles, and got each movie name, and yielded them into a dictionary of the actor’s name and the movies they’ve acted in."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC16B",
    "section": "",
    "text": "Image Classification Tutorial\n\n\n\n\n\n\nweek 8\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nMessage bank Tutorial\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2 - Scrapy\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Tutorial\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nAndrew Han\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nAndrew Han\n\n\n\n\n\n\nNo matching items"
  }
]